{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-E9qeg-6y1Hu"
   },
   "source": [
    "# LAB 3: How to setup a project from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "XHmVt4s034WK"
   },
   "outputs": [],
   "source": [
    "!rm -rf speech-emotion-recognition-25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6kJGhxzyN6d"
   },
   "source": [
    "# Step 1: Clone your project from Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Wfm084txMr0",
    "outputId": "91f0ad75-ff73-4cfb-b5f1-be42a8c96886"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'speech-emotion-recognition-25'...\n",
      "remote: Enumerating objects: 552, done.\u001b[K\n",
      "remote: Counting objects: 100% (123/123), done.\u001b[K\n",
      "remote: Compressing objects: 100% (91/91), done.\u001b[K\n",
      "remote: Total 552 (delta 79), reused 69 (delta 32), pack-reused 429 (from 3)\u001b[K\n",
      "Receiving objects: 100% (552/552), 3.24 MiB | 28.86 MiB/s, done.\n",
      "Resolving deltas: 100% (339/339), done.\n"
     ]
    }
   ],
   "source": [
    "#main\n",
    "#!git clone https://github.com/MatteoPaglia/speech-emotion-recognition-25.git\n",
    "\n",
    "#             nome branch\n",
    "\n",
    "!git clone -b RavdnessTrain https://github.com/MatteoPaglia/speech-emotion-recognition-25.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MN0lMneJxVz0",
    "outputId": "64b85f65-94db-4b58-9b9b-1e41effc2001"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints  eval.py\t       SpeechEmotionRecnognition.ipynb\twandb\n",
      "config.py    models\t       speech-emotion-recognition-25\n",
      "data\t     README.md\t       train.py\n",
      "dataset      requirements.txt  utils\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Pa5nOPxxbDf",
    "outputId": "cb19573e-9b7e-42c0-abe3-92b4fc6493de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/speech-emotion-recognition-25/speech-emotion-recognition-25\n"
     ]
    }
   ],
   "source": [
    "# %cd mldl_project_skeleton\n",
    "%cd speech-emotion-recognition-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VYilllpZzKMz",
    "outputId": "21364bda-49b5-450d-b782-846928e1bdd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints  dataset  README.md\t\t\t       train.py\n",
      "config.py    eval.py  requirements.txt\t\t       utils\n",
      "data\t     models   SpeechEmotionRecnognition.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be_4yDyp1Hru"
   },
   "source": [
    "# Step 2: Packages Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "EO9DuAYk1LFR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (1.7.4.5)\n",
      "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (0.3.13)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (1.6.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (2.9.0+cu126)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (2.9.0+cu126)\n",
      "Requirement already satisfied: torchcodec in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (0.9.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (0.24.0+cu126)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (0.11.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (2.0.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (3.10.0)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (0.13.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (4.67.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 13)) (11.3.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 14)) (6.0.3)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 15)) (0.23.1)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (6.3.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (2025.11.12)\n",
      "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (3.4.4)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (3.11)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (5.29.5)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (8.0.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (75.2.0)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (1.3)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (0.5.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub->-r requirements.txt (line 2)) (25.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 3)) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 3)) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 3)) (3.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (3.5.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 8)) (3.1.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 8)) (0.60.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 8)) (4.4.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 8)) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 8)) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 8)) (1.0.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 8)) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 8)) (1.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 10)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 10)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 10)) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 10)) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 10)) (3.2.5)\n",
      "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn->-r requirements.txt (line 11)) (2.2.2)\n",
      "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 15)) (8.3.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 15)) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 15)) (4.5.1)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 15)) (2.12.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 15)) (2.47.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 15)) (4.0.12)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa->-r requirements.txt (line 8)) (0.43.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn->-r requirements.txt (line 11)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn->-r requirements.txt (line 11)) (2025.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 15)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 15)) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 15)) (0.4.2)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa->-r requirements.txt (line 8)) (2.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r requirements.txt (line 4)) (3.0.3)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->-r requirements.txt (line 8)) (2.23)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 15)) (5.0.2)\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bifSi62Ixrqr"
   },
   "source": [
    "# Step 3: Dataset Setup\n",
    "## Different options\n",
    "- First one is downloading using a script that places the data in the download folder (usually recommended)\n",
    "- Second one is uploading the dataset to your personal/institutional Google Drive and load it from there ([Read More](https://saturncloud.io/blog/google-colab-how-to-read-data-from-my-google-drive/))\n",
    "- Place the download script directly here on colab\n",
    "\n",
    "You are free to do as you please in this phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "DiWQTaTbxeIc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Configurazione Kaggle ---\n",
      "Kaggle configurato con successo.\n",
      "\n",
      "--- Download RAVDESS ---\n",
      "Contatto KaggleHub per scaricare: uwrfkaggler/ravdess-emotional-speech-audio...\n",
      "Using Colab cache for faster access to the 'ravdess-emotional-speech-audio' dataset.\n",
      "‚úì Dataset scaricato nella cache di sistema: /kaggle/input/ravdess-emotional-speech-audio\n",
      "RAVDESS pronto in cache: /kaggle/input/ravdess-emotional-speech-audio\n",
      "Numero totale di file: 2880\n",
      "\n",
      "--- Download IEMOCAP ---\n",
      "Contatto KaggleHub per scaricare: dejolilandry/iemocapfullrelease...\n",
      "Using Colab cache for faster access to the 'iemocapfullrelease' dataset.\n",
      "‚úì Dataset scaricato nella cache di sistema: /kaggle/input/iemocapfullrelease\n",
      "IEMOCAP pronto in cache: /kaggle/input/iemocapfullrelease\n",
      "Numero totale di file: 81249\n",
      "\n",
      "============================================================\n",
      "RIEPILOGO DOWNLOAD\n",
      "============================================================\n",
      "RAVDESS: ‚úÖ Successo\n",
      "IEMOCAP: ‚úÖ Successo\n",
      "============================================================\n",
      "\n",
      "üéâ Tutti i dataset sono stati scaricati con successo!\n"
     ]
    }
   ],
   "source": [
    "!python utils/download_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Path diretti\n",
    "iemocap_path = Path('/kaggle/input/iemocapfullrelease/IEMOCAP_full_release')\n",
    "ravdess_path = Path('/kaggle/input/ravdess-emotional-speech-audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n\\nimport os\\nfrom pathlib import Path\\n\\nprint(\"=\"*80)\\nprint(\"üîç RICERCA PERCORSI DATASET\")\\nprint(\"=\"*80)\\n\\n# Percorsi possibili dove potrebbero essere i dataset\\npossible_paths = [\\n    Path(\\'/kaggle/input/\\'),\\n    Path.home() / \\'.cache\\' / \\'kagglehub\\' / \\'datasets\\',\\n    Path(\\'/root/.cache/kagglehub/datasets\\'),\\n    Path(\\'/tmp/kagglehub/datasets\\'),\\n    Path(\\'./data\\'),\\n    Path(\\'../data\\'),\\n    Path(\\'../../data\\'),\\n]\\n\\n# Aggiungi anche la directory corrente\\npossible_paths.append(Path.cwd())\\n\\nprint(f\"\\nüìÅ Directory corrente: {Path.cwd()}\\n\")\\n\\n# Ricerca IEMOCAP\\nprint(\"üîé Ricerca IEMOCAP_full_release...\")\\niemocap_path = None\\niemocap_found = False\\nfor base_path in possible_paths:\\n    if base_path.exists():\\n        for root, dirs, files in os.walk(base_path):\\n            if \\'IEMOCAP_full_release\\' in dirs:\\n                iemocap_path = Path(root) / \\'IEMOCAP_full_release\\'\\n                print(f\"‚úÖ IEMOCAP trovato a: {iemocap_path}\")\\n                iemocap_found = True\\n                break\\n    if iemocap_found:\\n        break\\n\\nif not iemocap_found:\\n    print(\"‚ùå IEMOCAP non trovato nei percorsi standard\")\\n    iemocap_path = None\\n\\n# Ricerca RAVDESS\\nprint(\"\\nüîé Ricerca ravdess-emotional-speech-audio...\")\\nravdess_path = None\\nravdess_found = False\\nfor base_path in possible_paths:\\n    if base_path.exists():\\n        for root, dirs, files in os.walk(base_path):\\n            if \\'ravdess-emotional-speech-audio\\' in dirs:\\n                ravdess_path = Path(root) / \\'ravdess-emotional-speech-audio\\'\\n                print(f\"‚úÖ RAVDESS trovato a: {ravdess_path}\")\\n                ravdess_found = True\\n                break\\n    if ravdess_found:\\n        break\\n\\nif not ravdess_found:\\n    print(\"‚ùå RAVDESS non trovato nei percorsi standard\")\\n    ravdess_path = None\\n\\n# Lista contenuti della directory data/ se esiste\\nprint(\"\\nüìÇ Contenuto della cartella \\'data/\\' (se presente):\")\\ndata_dir = Path(\\'./data\\')\\nif data_dir.exists():\\n    for item in data_dir.iterdir():\\n        print(f\"   - {item.name}\")\\nelse:\\n    print(\"   ‚ùå Cartella \\'data/\\' non trovata\")\\n\\nprint(\"\\n\" + \"=\"*80)\\nprint(\"‚úÖ VARIABILI SALVATE:\")\\nprint(f\"   - iemocap_path = {iemocap_path}\")\\nprint(f\"   - ravdess_path = {ravdess_path}\")\\nprint(\"=\"*80)\\n\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cerca e stampa i percorsi dei dataset\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üîç RICERCA PERCORSI DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Percorsi possibili dove potrebbero essere i dataset\n",
    "possible_paths = [\n",
    "    Path('/kaggle/input/'),\n",
    "    Path.home() / '.cache' / 'kagglehub' / 'datasets',\n",
    "    Path('/root/.cache/kagglehub/datasets'),\n",
    "    Path('/tmp/kagglehub/datasets'),\n",
    "    Path('./data'),\n",
    "    Path('../data'),\n",
    "    Path('../../data'),\n",
    "]\n",
    "\n",
    "# Aggiungi anche la directory corrente\n",
    "possible_paths.append(Path.cwd())\n",
    "\n",
    "print(f\"\\nüìÅ Directory corrente: {Path.cwd()}\\n\")\n",
    "\n",
    "# Ricerca IEMOCAP\n",
    "print(\"üîé Ricerca IEMOCAP_full_release...\")\n",
    "iemocap_path = None\n",
    "iemocap_found = False\n",
    "for base_path in possible_paths:\n",
    "    if base_path.exists():\n",
    "        for root, dirs, files in os.walk(base_path):\n",
    "            if 'IEMOCAP_full_release' in dirs:\n",
    "                iemocap_path = Path(root) / 'IEMOCAP_full_release'\n",
    "                print(f\"‚úÖ IEMOCAP trovato a: {iemocap_path}\")\n",
    "                iemocap_found = True\n",
    "                break\n",
    "    if iemocap_found:\n",
    "        break\n",
    "\n",
    "if not iemocap_found:\n",
    "    print(\"‚ùå IEMOCAP non trovato nei percorsi standard\")\n",
    "    iemocap_path = None\n",
    "\n",
    "# Ricerca RAVDESS\n",
    "print(\"\\nüîé Ricerca ravdess-emotional-speech-audio...\")\n",
    "ravdess_path = None\n",
    "ravdess_found = False\n",
    "for base_path in possible_paths:\n",
    "    if base_path.exists():\n",
    "        for root, dirs, files in os.walk(base_path):\n",
    "            if 'ravdess-emotional-speech-audio' in dirs:\n",
    "                ravdess_path = Path(root) / 'ravdess-emotional-speech-audio'\n",
    "                print(f\"‚úÖ RAVDESS trovato a: {ravdess_path}\")\n",
    "                ravdess_found = True\n",
    "                break\n",
    "    if ravdess_found:\n",
    "        break\n",
    "\n",
    "if not ravdess_found:\n",
    "    print(\"‚ùå RAVDESS non trovato nei percorsi standard\")\n",
    "    ravdess_path = None\n",
    "\n",
    "# Lista contenuti della directory data/ se esiste\n",
    "print(\"\\nüìÇ Contenuto della cartella 'data/' (se presente):\")\n",
    "data_dir = Path('./data')\n",
    "if data_dir.exists():\n",
    "    for item in data_dir.iterdir():\n",
    "        print(f\"   - {item.name}\")\n",
    "else:\n",
    "    print(\"   ‚ùå Cartella 'data/' non trovata\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ VARIABILI SALVATE:\")\n",
    "print(f\"   - iemocap_path = {iemocap_path}\")\n",
    "print(f\"   - ravdess_path = {ravdess_path}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' \\n\\nfrom torch.utils.data import DataLoader\\nfrom dataset.custom_iemocap_dataset import CustomIEMOCAPDataset\\nfrom dataset.custom_ravdess_dataset import CustomRAVDESSDataset\\n\\nprint(f\"////////////////////////////////////////////////////////////////////////////////////////////\")\\nprint(f\"Dataset IEMOCAP\")\\nprint(f\"////////////////////////////////////////////////////////////////////////////////////////////\")\\n\\n# Usa il percorso trovato in precedenza, altrimenti fallback\\nif iemocap_path and iemocap_path.exists():\\n    dataset_IEMOCAP_path = str(iemocap_path)\\n    print(f\"‚úÖ Usando percorso trovato: {dataset_IEMOCAP_path}\")\\nelse:\\n    dataset_IEMOCAP_path = \\'/kaggle/input/iemocapfullrelease/IEMOCAP_full_release\\'\\n    print(f\"‚ö†Ô∏è  Percorso non trovato, usando fallback: {dataset_IEMOCAP_path}\")\\n\\n# Create IEMOCAPdatasets\\ntrain_IEMOCAP_dataset = CustomIEMOCAPDataset(dataset_root=dataset_IEMOCAP_path, split=\\'train\\')\\nval_IEMOCAP_dataset = CustomIEMOCAPDataset(dataset_root=dataset_IEMOCAP_path, split=\\'validation\\')\\ntest_IEMOCAP_dataset = CustomIEMOCAPDataset(dataset_root=dataset_IEMOCAP_path, split=\\'test\\')\\n\\nprint(f\"Train samples: {len(train_IEMOCAP_dataset)}\")\\nprint(f\"Val samples: {len(val_IEMOCAP_dataset)}\")\\nprint(f\"Test samples: {len(test_IEMOCAP_dataset)}\")\\n\\n# Create IEMOCAP DataLoaders\\nbatch_size = 4\\ntrain_IEMOCAP_dataloader = DataLoader(train_IEMOCAP_dataset, batch_size=batch_size, shuffle=True)\\nval_IEMOCAP_dataloader = DataLoader(val_IEMOCAP_dataset, batch_size=batch_size, shuffle=False)\\ntest_IEMOCAP_dataloader = DataLoader(test_IEMOCAP_dataset, batch_size=batch_size, shuffle=False)\\n\\n\\nprint(f\"////////////////////////////////////////////////////////////////////////////////////////////\")\\nprint(f\"Dataset RAVDESS\")\\nprint(f\"////////////////////////////////////////////////////////////////////////////////////////////\")\\n\\n# Usa il percorso trovato in precedenza, altrimenti fallback\\nif ravdess_path and ravdess_path.exists():\\n    dataset_RAVDESS_path = str(ravdess_path)\\n    print(f\"‚úÖ Usando percorso trovato: {dataset_RAVDESS_path}\")\\nelse:\\n    dataset_RAVDESS_path = \\'/kaggle/input/ravdess-emotional-speech-audio\\'\\n    print(f\"‚ö†Ô∏è  Percorso non trovato, usando fallback: {dataset_RAVDESS_path}\")\\n\\n# Create RAVDESS datasets\\ntrain_RAVDESS_dataset = CustomRAVDESSDataset(dataset_root=dataset_RAVDESS_path, split=\\'train\\')\\nval_RAVDESS_dataset = CustomRAVDESSDataset(dataset_root=dataset_RAVDESS_path, split=\\'validation\\')\\ntest_RAVDESS_dataset = CustomRAVDESSDataset(dataset_root=dataset_RAVDESS_path, split=\\'test\\')\\n\\nprint(f\"Train samples: {len(train_RAVDESS_dataset)}\")\\nprint(f\"Val samples: {len(val_RAVDESS_dataset)}\")\\nprint(f\"Test samples: {len(test_RAVDESS_dataset)}\")\\n\\n# Create RAVDESS DataLoaders\\nbatch_size = 4\\ntrain_RAVDESS_dataloader = DataLoader(train_RAVDESS_dataset, batch_size=batch_size, shuffle=True)\\nval_RAVDESS_dataloader = DataLoader(val_RAVDESS_dataset, batch_size=batch_size, shuffle=False)\\ntest_RAVDESS_dataloader = DataLoader(test_RAVDESS_dataset, batch_size=batch_size, shuffle=False)\\n\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crea dei DataLoader per stampare le statisiche \n",
    "\n",
    "#TODO:  #creare un file per stampare le statistiche utili e toglierle dalle classi\n",
    "# √® necessario creare dei DataLoader per poter calcolare le statistiche sui dataset? \n",
    "\n",
    "\"\"\" \n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset.custom_iemocap_dataset import CustomIEMOCAPDataset\n",
    "from dataset.custom_ravdess_dataset import CustomRAVDESSDataset\n",
    "\n",
    "print(f\"////////////////////////////////////////////////////////////////////////////////////////////\")\n",
    "print(f\"Dataset IEMOCAP\")\n",
    "print(f\"////////////////////////////////////////////////////////////////////////////////////////////\")\n",
    "\n",
    "# Usa il percorso trovato in precedenza, altrimenti fallback\n",
    "if iemocap_path and iemocap_path.exists():\n",
    "    dataset_IEMOCAP_path = str(iemocap_path)\n",
    "    print(f\"‚úÖ Usando percorso trovato: {dataset_IEMOCAP_path}\")\n",
    "else:\n",
    "    dataset_IEMOCAP_path = '/kaggle/input/iemocapfullrelease/IEMOCAP_full_release'\n",
    "    print(f\"‚ö†Ô∏è  Percorso non trovato, usando fallback: {dataset_IEMOCAP_path}\")\n",
    "\n",
    "# Create IEMOCAPdatasets\n",
    "train_IEMOCAP_dataset = CustomIEMOCAPDataset(dataset_root=dataset_IEMOCAP_path, split='train')\n",
    "val_IEMOCAP_dataset = CustomIEMOCAPDataset(dataset_root=dataset_IEMOCAP_path, split='validation')\n",
    "test_IEMOCAP_dataset = CustomIEMOCAPDataset(dataset_root=dataset_IEMOCAP_path, split='test')\n",
    "\n",
    "print(f\"Train samples: {len(train_IEMOCAP_dataset)}\")\n",
    "print(f\"Val samples: {len(val_IEMOCAP_dataset)}\")\n",
    "print(f\"Test samples: {len(test_IEMOCAP_dataset)}\")\n",
    "\n",
    "# Create IEMOCAP DataLoaders\n",
    "batch_size = 4\n",
    "train_IEMOCAP_dataloader = DataLoader(train_IEMOCAP_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_IEMOCAP_dataloader = DataLoader(val_IEMOCAP_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_IEMOCAP_dataloader = DataLoader(test_IEMOCAP_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "print(f\"////////////////////////////////////////////////////////////////////////////////////////////\")\n",
    "print(f\"Dataset RAVDESS\")\n",
    "print(f\"////////////////////////////////////////////////////////////////////////////////////////////\")\n",
    "\n",
    "# Usa il percorso trovato in precedenza, altrimenti fallback\n",
    "if ravdess_path and ravdess_path.exists():\n",
    "    dataset_RAVDESS_path = str(ravdess_path)\n",
    "    print(f\"‚úÖ Usando percorso trovato: {dataset_RAVDESS_path}\")\n",
    "else:\n",
    "    dataset_RAVDESS_path = '/kaggle/input/ravdess-emotional-speech-audio'\n",
    "    print(f\"‚ö†Ô∏è  Percorso non trovato, usando fallback: {dataset_RAVDESS_path}\")\n",
    "\n",
    "# Create RAVDESS datasets\n",
    "train_RAVDESS_dataset = CustomRAVDESSDataset(dataset_root=dataset_RAVDESS_path, split='train')\n",
    "val_RAVDESS_dataset = CustomRAVDESSDataset(dataset_root=dataset_RAVDESS_path, split='validation')\n",
    "test_RAVDESS_dataset = CustomRAVDESSDataset(dataset_root=dataset_RAVDESS_path, split='test')\n",
    "\n",
    "print(f\"Train samples: {len(train_RAVDESS_dataset)}\")\n",
    "print(f\"Val samples: {len(val_RAVDESS_dataset)}\")\n",
    "print(f\"Test samples: {len(test_RAVDESS_dataset)}\")\n",
    "\n",
    "# Create RAVDESS DataLoaders\n",
    "batch_size = 4\n",
    "train_RAVDESS_dataloader = DataLoader(train_RAVDESS_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_RAVDESS_dataloader = DataLoader(val_RAVDESS_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_RAVDESS_dataloader = DataLoader(test_RAVDESS_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' import matplotlib.pyplot as plt\\nimport matplotlib.patches as mpatches\\n\\n# Colori per le emozioni\\nemotion_colors = {\\n    \\'neutral\\': \\'#4285F4\\',  # Blu\\n    \\'happy\\': \\'#34A853\\',    # Verde\\n    \\'sad\\': \\'#EA4335\\',      # Rosso\\n    \\'angry\\': \\'#FBBC04\\'     # Giallo/Arancione\\n}\\n\\nfig, axes = plt.subplots(3, 2, figsize=(16, 14))\\nfig.suptitle(\\'Log-Mel Spectrograms: IEMOCAP (Train vs Validation) vs RAVDESS (Train)\\', fontsize=16, fontweight=\\'bold\\')\\n\\n# ===== IEMOCAP - VALIDATION (ha le label) =====\\nprint(\"Loading IEMOCAP VALIDATION samples...\")\\niemocap_val_sample_1 = val_IEMOCAP_dataset[0]\\niemocap_val_sample_2 = val_IEMOCAP_dataset[1]\\n\\n# Plot IEMOCAP Validation Sample 1\\nim1 = axes[0, 0].imshow(iemocap_val_sample_1[\\'audio_features\\'].squeeze().numpy(), \\n                        aspect=\\'auto\\', origin=\\'lower\\', cmap=\\'viridis\\')\\naxes[0, 0].set_title(f\"IEMOCAP VALIDATION Sample 1\\nEmotion: {iemocap_val_sample_1[\\'emotion\\']} | Actor: {iemocap_val_sample_1[\\'actor_id\\']}\", \\n                     fontweight=\\'bold\\', color=\\'green\\')\\naxes[0, 0].set_ylabel(\\'Mel Frequency Bins\\')\\naxes[0, 0].set_xlabel(\\'Time Frames\\')\\nplt.colorbar(im1, ax=axes[0, 0], label=\\'dB\\')\\n\\n# Plot IEMOCAP Validation Sample 2\\nim2 = axes[0, 1].imshow(iemocap_val_sample_2[\\'audio_features\\'].squeeze().numpy(), \\n                        aspect=\\'auto\\', origin=\\'lower\\', cmap=\\'viridis\\')\\naxes[0, 1].set_title(f\"IEMOCAP VALIDATION Sample 2\\nEmotion: {iemocap_val_sample_2[\\'emotion\\']} | Actor: {iemocap_val_sample_2[\\'actor_id\\']}\", \\n                     fontweight=\\'bold\\', color=\\'green\\')\\naxes[0, 1].set_ylabel(\\'Mel Frequency Bins\\')\\naxes[0, 1].set_xlabel(\\'Time Frames\\')\\nplt.colorbar(im2, ax=axes[0, 1], label=\\'dB\\')\\n\\n# ===== IEMOCAP - TRAIN (NO label) =====\\nprint(\"Loading IEMOCAP TRAIN samples...\")\\niemocap_train_sample_1 = train_IEMOCAP_dataset[0]\\niemocap_train_sample_2 = train_IEMOCAP_dataset[1]\\n\\n# Plot IEMOCAP Train Sample 1\\nim3 = axes[1, 0].imshow(iemocap_train_sample_1[\\'audio_features\\'].squeeze().numpy(), \\n                        aspect=\\'auto\\', origin=\\'lower\\', cmap=\\'viridis\\')\\naxes[1, 0].set_title(f\"IEMOCAP TRAIN Sample 1\\nEmotion: {iemocap_train_sample_1[\\'emotion\\']} (NO LABEL) | Actor: {iemocap_train_sample_1[\\'actor_id\\']}\", \\n                     fontweight=\\'bold\\', color=\\'red\\')\\naxes[1, 0].set_ylabel(\\'Mel Frequency Bins\\')\\naxes[1, 0].set_xlabel(\\'Time Frames\\')\\nplt.colorbar(im3, ax=axes[1, 0], label=\\'dB\\')\\n\\n# Plot IEMOCAP Train Sample 2\\nim4 = axes[1, 1].imshow(iemocap_train_sample_2[\\'audio_features\\'].squeeze().numpy(), \\n                        aspect=\\'auto\\', origin=\\'lower\\', cmap=\\'viridis\\')\\naxes[1, 1].set_title(f\"IEMOCAP TRAIN Sample 2\\nEmotion: {iemocap_train_sample_2[\\'emotion\\']} (NO LABEL) | Actor: {iemocap_train_sample_2[\\'actor_id\\']}\", \\n                     fontweight=\\'bold\\', color=\\'red\\')\\naxes[1, 1].set_ylabel(\\'Mel Frequency Bins\\')\\naxes[1, 1].set_xlabel(\\'Time Frames\\')\\nplt.colorbar(im4, ax=axes[1, 1], label=\\'dB\\')\\n\\n# ===== RAVDESS - TRAIN =====\\nprint(\"Loading RAVDESS TRAIN samples...\")\\nravdess_sample_1 = train_RAVDESS_dataset[0]\\nravdess_sample_2 = train_RAVDESS_dataset[1]\\n\\n# Plot RAVDESS Sample 1\\nim5 = axes[2, 0].imshow(ravdess_sample_1[\\'audio_features\\'].squeeze().numpy(), \\n                        aspect=\\'auto\\', origin=\\'lower\\', cmap=\\'viridis\\')\\naxes[2, 0].set_title(f\"RAVDESS TRAIN Sample 1\\nEmotion: {ravdess_sample_1[\\'emotion\\']} | Actor: {ravdess_sample_1[\\'actor_id\\']}\", \\n                     fontweight=\\'bold\\')\\naxes[2, 0].set_ylabel(\\'Mel Frequency Bins\\')\\naxes[2, 0].set_xlabel(\\'Time Frames\\')\\nplt.colorbar(im5, ax=axes[2, 0], label=\\'dB\\')\\n\\n# Plot RAVDESS Sample 2\\nim6 = axes[2, 1].imshow(ravdess_sample_2[\\'audio_features\\'].squeeze().numpy(), \\n                        aspect=\\'auto\\', origin=\\'lower\\', cmap=\\'viridis\\')\\naxes[2, 1].set_title(f\"RAVDESS TRAIN Sample 2\\nEmotion: {ravdess_sample_2[\\'emotion\\']} | Actor: {ravdess_sample_2[\\'actor_id\\']}\", \\n                     fontweight=\\'bold\\')\\naxes[2, 1].set_ylabel(\\'Mel Frequency Bins\\')\\naxes[2, 1].set_xlabel(\\'Time Frames\\')\\nplt.colorbar(im6, ax=axes[2, 1], label=\\'dB\\')\\n\\nplt.tight_layout()\\nplt.show()\\n\\n# ===== Stampa delle statistiche =====\\nprint(\"\\n\" + \"=\"*80)\\nprint(\"SAMPLE DETAILS\")\\nprint(\"=\"*80)\\n\\nprint(\"\\n‚úÖ IEMOCAP VALIDATION (con label):\")\\nprint(\"\\nüìä Sample 1:\")\\nprint(f\"   Emotion: {iemocap_val_sample_1[\\'emotion\\']} (ID: {iemocap_val_sample_1[\\'emotion_id\\']})\")\\nprint(f\"   Actor: {iemocap_val_sample_1[\\'actor_id\\']}\")\\nprint(f\"   Spectrogram Shape: {iemocap_val_sample_1[\\'audio_features\\'].shape} (channels, mel_bins, time_frames)\")\\nprint(f\"   Min value: {iemocap_val_sample_1[\\'audio_features\\'].min().item():.2f} dB\")\\nprint(f\"   Max value: {iemocap_val_sample_1[\\'audio_features\\'].max().item():.2f} dB\")\\nprint(f\"   Mean value: {iemocap_val_sample_1[\\'audio_features\\'].mean().item():.2f} dB\")\\n\\nprint(\"\\nüìä Sample 2:\")\\nprint(f\"   Emotion: {iemocap_val_sample_2[\\'emotion\\']} (ID: {iemocap_val_sample_2[\\'emotion_id\\']})\")\\nprint(f\"   Actor: {iemocap_val_sample_2[\\'actor_id\\']}\")\\nprint(f\"   Spectrogram Shape: {iemocap_val_sample_2[\\'audio_features\\'].shape}\")\\nprint(f\"   Min value: {iemocap_val_sample_2[\\'audio_features\\'].min().item():.2f} dB\")\\nprint(f\"   Max value: {iemocap_val_sample_2[\\'audio_features\\'].max().item():.2f} dB\")\\nprint(f\"   Mean value: {iemocap_val_sample_2[\\'audio_features\\'].mean().item():.2f} dB\")\\n\\nprint(\"\\n‚ùå IEMOCAP TRAIN (SENZA label - Unsupervised):\")\\nprint(\"\\nüìä Sample 1:\")\\nprint(f\"   Emotion: {iemocap_train_sample_1[\\'emotion\\']} | Audio ID: {iemocap_train_sample_1[\\'emotion_id\\']}\")\\nprint(f\"   Actor: {iemocap_train_sample_1[\\'actor_id\\']}\")\\nprint(f\"   Spectrogram Shape: {iemocap_train_sample_1[\\'audio_features\\'].shape}\")\\nprint(f\"   Min value: {iemocap_train_sample_1[\\'audio_features\\'].min().item():.2f} dB\")\\nprint(f\"   Max value: {iemocap_train_sample_1[\\'audio_features\\'].max().item():.2f} dB\")\\nprint(f\"   Mean value: {iemocap_train_sample_1[\\'audio_features\\'].mean().item():.2f} dB\")\\n\\nprint(\"\\nüìä Sample 2:\")\\nprint(f\"   Emotion: {iemocap_train_sample_2[\\'emotion\\']} | Audio ID: {iemocap_train_sample_2[\\'emotion_id\\']}\")\\nprint(f\"   Actor: {iemocap_train_sample_2[\\'actor_id\\']}\")\\nprint(f\"   Spectrogram Shape: {iemocap_train_sample_2[\\'audio_features\\'].shape}\")\\nprint(f\"   Min value: {iemocap_train_sample_2[\\'audio_features\\'].min().item():.2f} dB\")\\nprint(f\"   Max value: {iemocap_train_sample_2[\\'audio_features\\'].max().item():.2f} dB\")\\nprint(f\"   Mean value: {iemocap_train_sample_2[\\'audio_features\\'].mean().item():.2f} dB\")\\n\\nprint(\"\\n‚úÖ RAVDESS TRAIN (con label):\")\\nprint(\"\\nüìä Sample 1:\")\\nprint(f\"   Emotion: {ravdess_sample_1[\\'emotion\\']} (ID: {ravdess_sample_1[\\'emotion_id\\']})\")\\nprint(f\"   Actor: {ravdess_sample_1[\\'actor_id\\']}\")\\nprint(f\"   Spectrogram Shape: {ravdess_sample_1[\\'audio_features\\'].shape}\")\\nprint(f\"   Min value: {ravdess_sample_1[\\'audio_features\\'].min().item():.2f} dB\")\\nprint(f\"   Max value: {ravdess_sample_1[\\'audio_features\\'].max().item():.2f} dB\")\\nprint(f\"   Mean value: {ravdess_sample_1[\\'audio_features\\'].mean().item():.2f} dB\")\\n\\nprint(\"\\nüìä Sample 2:\")\\nprint(f\"   Emotion: {ravdess_sample_2[\\'emotion\\']} (ID: {ravdess_sample_2[\\'emotion_id\\']})\")\\nprint(f\"   Actor: {ravdess_sample_2[\\'actor_id\\']}\")\\nprint(f\"   Spectrogram Shape: {ravdess_sample_2[\\'audio_features\\'].shape}\")\\nprint(f\"   Min value: {ravdess_sample_2[\\'audio_features\\'].min().item():.2f} dB\")\\nprint(f\"   Max value: {ravdess_sample_2[\\'audio_features\\'].max().item():.2f} dB\")\\nprint(f\"   Mean value: {ravdess_sample_2[\\'audio_features\\'].mean().item():.2f} dB\")\\n\\nprint(\"\\n\" + \"=\"*80)\\nprint(\"‚úÖ Datasets are ready for training!\")\\nprint(\"=\"*80) '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizza alcuni spettri di log-mel e stampa le statistiche dei dataset\n",
    "\n",
    "\"\"\" import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Colori per le emozioni\n",
    "emotion_colors = {\n",
    "    'neutral': '#4285F4',  # Blu\n",
    "    'happy': '#34A853',    # Verde\n",
    "    'sad': '#EA4335',      # Rosso\n",
    "    'angry': '#FBBC04'     # Giallo/Arancione\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 14))\n",
    "fig.suptitle('Log-Mel Spectrograms: IEMOCAP (Train vs Validation) vs RAVDESS (Train)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# ===== IEMOCAP - VALIDATION (ha le label) =====\n",
    "print(\"Loading IEMOCAP VALIDATION samples...\")\n",
    "iemocap_val_sample_1 = val_IEMOCAP_dataset[0]\n",
    "iemocap_val_sample_2 = val_IEMOCAP_dataset[1]\n",
    "\n",
    "# Plot IEMOCAP Validation Sample 1\n",
    "im1 = axes[0, 0].imshow(iemocap_val_sample_1['audio_features'].squeeze().numpy(), \n",
    "                        aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[0, 0].set_title(f\"IEMOCAP VALIDATION Sample 1\\nEmotion: {iemocap_val_sample_1['emotion']} | Actor: {iemocap_val_sample_1['actor_id']}\", \n",
    "                     fontweight='bold', color='green')\n",
    "axes[0, 0].set_ylabel('Mel Frequency Bins')\n",
    "axes[0, 0].set_xlabel('Time Frames')\n",
    "plt.colorbar(im1, ax=axes[0, 0], label='dB')\n",
    "\n",
    "# Plot IEMOCAP Validation Sample 2\n",
    "im2 = axes[0, 1].imshow(iemocap_val_sample_2['audio_features'].squeeze().numpy(), \n",
    "                        aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[0, 1].set_title(f\"IEMOCAP VALIDATION Sample 2\\nEmotion: {iemocap_val_sample_2['emotion']} | Actor: {iemocap_val_sample_2['actor_id']}\", \n",
    "                     fontweight='bold', color='green')\n",
    "axes[0, 1].set_ylabel('Mel Frequency Bins')\n",
    "axes[0, 1].set_xlabel('Time Frames')\n",
    "plt.colorbar(im2, ax=axes[0, 1], label='dB')\n",
    "\n",
    "# ===== IEMOCAP - TRAIN (NO label) =====\n",
    "print(\"Loading IEMOCAP TRAIN samples...\")\n",
    "iemocap_train_sample_1 = train_IEMOCAP_dataset[0]\n",
    "iemocap_train_sample_2 = train_IEMOCAP_dataset[1]\n",
    "\n",
    "# Plot IEMOCAP Train Sample 1\n",
    "im3 = axes[1, 0].imshow(iemocap_train_sample_1['audio_features'].squeeze().numpy(), \n",
    "                        aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[1, 0].set_title(f\"IEMOCAP TRAIN Sample 1\\nEmotion: {iemocap_train_sample_1['emotion']} (NO LABEL) | Actor: {iemocap_train_sample_1['actor_id']}\", \n",
    "                     fontweight='bold', color='red')\n",
    "axes[1, 0].set_ylabel('Mel Frequency Bins')\n",
    "axes[1, 0].set_xlabel('Time Frames')\n",
    "plt.colorbar(im3, ax=axes[1, 0], label='dB')\n",
    "\n",
    "# Plot IEMOCAP Train Sample 2\n",
    "im4 = axes[1, 1].imshow(iemocap_train_sample_2['audio_features'].squeeze().numpy(), \n",
    "                        aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[1, 1].set_title(f\"IEMOCAP TRAIN Sample 2\\nEmotion: {iemocap_train_sample_2['emotion']} (NO LABEL) | Actor: {iemocap_train_sample_2['actor_id']}\", \n",
    "                     fontweight='bold', color='red')\n",
    "axes[1, 1].set_ylabel('Mel Frequency Bins')\n",
    "axes[1, 1].set_xlabel('Time Frames')\n",
    "plt.colorbar(im4, ax=axes[1, 1], label='dB')\n",
    "\n",
    "# ===== RAVDESS - TRAIN =====\n",
    "print(\"Loading RAVDESS TRAIN samples...\")\n",
    "ravdess_sample_1 = train_RAVDESS_dataset[0]\n",
    "ravdess_sample_2 = train_RAVDESS_dataset[1]\n",
    "\n",
    "# Plot RAVDESS Sample 1\n",
    "im5 = axes[2, 0].imshow(ravdess_sample_1['audio_features'].squeeze().numpy(), \n",
    "                        aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[2, 0].set_title(f\"RAVDESS TRAIN Sample 1\\nEmotion: {ravdess_sample_1['emotion']} | Actor: {ravdess_sample_1['actor_id']}\", \n",
    "                     fontweight='bold')\n",
    "axes[2, 0].set_ylabel('Mel Frequency Bins')\n",
    "axes[2, 0].set_xlabel('Time Frames')\n",
    "plt.colorbar(im5, ax=axes[2, 0], label='dB')\n",
    "\n",
    "# Plot RAVDESS Sample 2\n",
    "im6 = axes[2, 1].imshow(ravdess_sample_2['audio_features'].squeeze().numpy(), \n",
    "                        aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[2, 1].set_title(f\"RAVDESS TRAIN Sample 2\\nEmotion: {ravdess_sample_2['emotion']} | Actor: {ravdess_sample_2['actor_id']}\", \n",
    "                     fontweight='bold')\n",
    "axes[2, 1].set_ylabel('Mel Frequency Bins')\n",
    "axes[2, 1].set_xlabel('Time Frames')\n",
    "plt.colorbar(im6, ax=axes[2, 1], label='dB')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===== Stampa delle statistiche =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE DETAILS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n‚úÖ IEMOCAP VALIDATION (con label):\")\n",
    "print(\"\\nüìä Sample 1:\")\n",
    "print(f\"   Emotion: {iemocap_val_sample_1['emotion']} (ID: {iemocap_val_sample_1['emotion_id']})\")\n",
    "print(f\"   Actor: {iemocap_val_sample_1['actor_id']}\")\n",
    "print(f\"   Spectrogram Shape: {iemocap_val_sample_1['audio_features'].shape} (channels, mel_bins, time_frames)\")\n",
    "print(f\"   Min value: {iemocap_val_sample_1['audio_features'].min().item():.2f} dB\")\n",
    "print(f\"   Max value: {iemocap_val_sample_1['audio_features'].max().item():.2f} dB\")\n",
    "print(f\"   Mean value: {iemocap_val_sample_1['audio_features'].mean().item():.2f} dB\")\n",
    "\n",
    "print(\"\\nüìä Sample 2:\")\n",
    "print(f\"   Emotion: {iemocap_val_sample_2['emotion']} (ID: {iemocap_val_sample_2['emotion_id']})\")\n",
    "print(f\"   Actor: {iemocap_val_sample_2['actor_id']}\")\n",
    "print(f\"   Spectrogram Shape: {iemocap_val_sample_2['audio_features'].shape}\")\n",
    "print(f\"   Min value: {iemocap_val_sample_2['audio_features'].min().item():.2f} dB\")\n",
    "print(f\"   Max value: {iemocap_val_sample_2['audio_features'].max().item():.2f} dB\")\n",
    "print(f\"   Mean value: {iemocap_val_sample_2['audio_features'].mean().item():.2f} dB\")\n",
    "\n",
    "print(\"\\n‚ùå IEMOCAP TRAIN (SENZA label - Unsupervised):\")\n",
    "print(\"\\nüìä Sample 1:\")\n",
    "print(f\"   Emotion: {iemocap_train_sample_1['emotion']} | Audio ID: {iemocap_train_sample_1['emotion_id']}\")\n",
    "print(f\"   Actor: {iemocap_train_sample_1['actor_id']}\")\n",
    "print(f\"   Spectrogram Shape: {iemocap_train_sample_1['audio_features'].shape}\")\n",
    "print(f\"   Min value: {iemocap_train_sample_1['audio_features'].min().item():.2f} dB\")\n",
    "print(f\"   Max value: {iemocap_train_sample_1['audio_features'].max().item():.2f} dB\")\n",
    "print(f\"   Mean value: {iemocap_train_sample_1['audio_features'].mean().item():.2f} dB\")\n",
    "\n",
    "print(\"\\nüìä Sample 2:\")\n",
    "print(f\"   Emotion: {iemocap_train_sample_2['emotion']} | Audio ID: {iemocap_train_sample_2['emotion_id']}\")\n",
    "print(f\"   Actor: {iemocap_train_sample_2['actor_id']}\")\n",
    "print(f\"   Spectrogram Shape: {iemocap_train_sample_2['audio_features'].shape}\")\n",
    "print(f\"   Min value: {iemocap_train_sample_2['audio_features'].min().item():.2f} dB\")\n",
    "print(f\"   Max value: {iemocap_train_sample_2['audio_features'].max().item():.2f} dB\")\n",
    "print(f\"   Mean value: {iemocap_train_sample_2['audio_features'].mean().item():.2f} dB\")\n",
    "\n",
    "print(\"\\n‚úÖ RAVDESS TRAIN (con label):\")\n",
    "print(\"\\nüìä Sample 1:\")\n",
    "print(f\"   Emotion: {ravdess_sample_1['emotion']} (ID: {ravdess_sample_1['emotion_id']})\")\n",
    "print(f\"   Actor: {ravdess_sample_1['actor_id']}\")\n",
    "print(f\"   Spectrogram Shape: {ravdess_sample_1['audio_features'].shape}\")\n",
    "print(f\"   Min value: {ravdess_sample_1['audio_features'].min().item():.2f} dB\")\n",
    "print(f\"   Max value: {ravdess_sample_1['audio_features'].max().item():.2f} dB\")\n",
    "print(f\"   Mean value: {ravdess_sample_1['audio_features'].mean().item():.2f} dB\")\n",
    "\n",
    "print(\"\\nüìä Sample 2:\")\n",
    "print(f\"   Emotion: {ravdess_sample_2['emotion']} (ID: {ravdess_sample_2['emotion_id']})\")\n",
    "print(f\"   Actor: {ravdess_sample_2['actor_id']}\")\n",
    "print(f\"   Spectrogram Shape: {ravdess_sample_2['audio_features'].shape}\")\n",
    "print(f\"   Min value: {ravdess_sample_2['audio_features'].min().item():.2f} dB\")\n",
    "print(f\"   Max value: {ravdess_sample_2['audio_features'].max().item():.2f} dB\")\n",
    "print(f\"   Mean value: {ravdess_sample_2['audio_features'].mean().item():.2f} dB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Datasets are ready for training!\")\n",
    "print(\"=\"*80) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqo9Eh79yihI"
   },
   "source": [
    "# Step 4: Train your model and visualize training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Weights & Biases : Genera i grafici e compara gli esperimenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "4-dxDQOFcdgX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ['WANDB_API_KEY'] = '7ade30086de7899bed412e3eb5c2da065c146f90'\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "id": "8q9OvEDHxmRv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Classe SimpleEarlyStopping pronta!\n",
      "Using device: cuda\n",
      "\n",
      "‚úÖ RAVDESS trovato: /kaggle/input/ravdess-emotional-speech-audio\n",
      "\n",
      "üìä Statistiche del dataset RAVDESS:\n",
      "\n",
      "========================================\n",
      "üìä ANALISI RAVDESS TRAINING SET\n",
      "========================================\n",
      "üîπ Samples Totali: 1440\n",
      "üîπ Attori (20): [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "   - Maschi:  10\n",
      "   - Femmine: 10\n",
      "\n",
      "üé≠ Distribuzione Emozioni:\n",
      "   - Angry     :  320 ( 22.2%) ‚ñà‚ñà‚ñà‚ñà\n",
      "   - Happy     :  320 ( 22.2%) ‚ñà‚ñà‚ñà‚ñà\n",
      "   - Neutral   :  480 ( 33.3%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   - Sad       :  320 ( 22.2%) ‚ñà‚ñà‚ñà‚ñà\n",
      "----------------------------------------\n",
      "üìä Statistiche del dataset RAVDESS:\n",
      "\n",
      "========================================\n",
      "üìä ANALISI RAVDESS VALIDATION SET\n",
      "========================================\n",
      "üîπ Samples Totali: 144\n",
      "üîπ Attori (2): [21, 22]\n",
      "   - Maschi:  1\n",
      "   - Femmine: 1\n",
      "\n",
      "üé≠ Distribuzione Emozioni:\n",
      "   - Angry     :   32 ( 22.2%) ‚ñà‚ñà‚ñà‚ñà\n",
      "   - Happy     :   32 ( 22.2%) ‚ñà‚ñà‚ñà‚ñà\n",
      "   - Neutral   :   48 ( 33.3%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   - Sad       :   32 ( 22.2%) ‚ñà‚ñà‚ñà‚ñà\n",
      "----------------------------------------\n",
      "Train samples: 1440\n",
      "Val samples: 144\n",
      "\n",
      "================================================================================\n",
      "üèóÔ∏è ARCHITETTURA DEL MODELLO\n",
      "================================================================================\n",
      "CRNN_BiLSTM(\n",
      "  (block1): Sequential(\n",
      "    (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (block2): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (block3): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (block4): Sequential(\n",
      "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (projection): Linear(in_features=8192, out_features=128, bias=True)\n",
      "  (lstm): LSTM(128, 128, batch_first=True, bidirectional=True)\n",
      "  (attention_linear): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (classifier): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n",
      "================================================================================\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpagliarellomatteo\u001b[0m (\u001b[33mpagliarellomatteo-politecnico-di-torino\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/speech-emotion-recognition-25/speech-emotion-recognition-25/wandb/run-20260106_154902-e4q1nboi\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrain_20260106_164902\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/pagliarellomatteo-politecnico-di-torino/speech-emotion-recognition\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pagliarellomatteo-politecnico-di-torino/speech-emotion-recognition/runs/e4q1nboi\u001b[0m\n",
      "\n",
      "================================================================================\n",
      "üîß IPERPARAMETRI DI TRAINING\n",
      "================================================================================\n",
      "Device:                cuda\n",
      "Batch Size:            32\n",
      "Learning Rate:         0.0005\n",
      "Weight Decay (L2):     0.001\n",
      "Number of Epochs:      100\n",
      "Early Stopping Patience: 10\n",
      "\n",
      "Modello:\n",
      "  - Num Classes:       4\n",
      "  - Time Steps:        200\n",
      "  - Mel Bands:         128\n",
      "\n",
      "Optimizer:             Adam\n",
      "Loss Function:         CrossEntropyLoss\n",
      "Train Samples:         1440\n",
      "Val Samples:           144\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "Training:   0% 0/45 [00:00<?, ?it/s]\n",
      "üìä Calcolando statistiche durata POST-TRIMMING per split 'train'...\n",
      "   288/1440 file processati...\n",
      "   576/1440 file processati...\n",
      "   864/1440 file processati...\n",
      "   1152/1440 file processati...\n",
      "   1440/1440 file processati...\n",
      "‚úÖ Media: 2.19s (35043 campioni)\n",
      "‚úÖ Massimo: 4.40s (70471 campioni)\n",
      "\n",
      "                                                        \n",
      "üìä Calcolando statistiche durata POST-TRIMMING per split 'validation'...\n",
      "   28/144 file processati...\n",
      "   56/144 file processati...\n",
      "   84/144 file processati...\n",
      "   112/144 file processati...\n",
      "   140/144 file processati...\n",
      "‚úÖ Media: 2.22s (35541 campioni)\n",
      "‚úÖ Massimo: 3.07s (49152 campioni)\n",
      "\n",
      "Train Loss: 1.3010 | Train Acc: 38.54%\n",
      "Val Loss:   1.4221 | Val Acc:   36.11%\n",
      ">>> Model Saved!\n",
      "üìä Best val loss: 1.4221\n",
      "\n",
      "Epoch 2/100\n",
      "Train Loss: 1.1658 | Train Acc: 49.24%                   \n",
      "Val Loss:   1.0928 | Val Acc:   58.33%\n",
      ">>> Model Saved!\n",
      "‚úì Migliorato! New best: 1.0928\n",
      "\n",
      "Epoch 3/100\n",
      "Train Loss: 1.1152 | Train Acc: 52.85%                   \n",
      "Val Loss:   1.0247 | Val Acc:   50.00%\n",
      "‚úì Migliorato! New best: 1.0247\n",
      "\n",
      "Epoch 4/100\n",
      "Train Loss: 1.0953 | Train Acc: 54.79%                   \n",
      "Val Loss:   1.0035 | Val Acc:   61.11%\n",
      ">>> Model Saved!\n",
      "‚úì Migliorato! New best: 1.0035\n",
      "\n",
      "Epoch 5/100\n",
      "Train Loss: 1.0471 | Train Acc: 58.75%                   \n",
      "Val Loss:   1.0600 | Val Acc:   61.11%\n",
      "‚ö†Ô∏è Nessun miglioramento (1/10)\n",
      "\n",
      "Epoch 6/100\n",
      "Train Loss: 0.9960 | Train Acc: 60.69%                   \n",
      "Val Loss:   1.2876 | Val Acc:   34.72%\n",
      "‚ö†Ô∏è Nessun miglioramento (2/10)\n",
      "\n",
      "Epoch 7/100\n",
      "Train Loss: 0.9985 | Train Acc: 62.71%                   \n",
      "Val Loss:   1.0160 | Val Acc:   45.83%\n",
      "‚ö†Ô∏è Nessun miglioramento (3/10)\n",
      "\n",
      "Epoch 8/100\n",
      "Train Loss: 0.9863 | Train Acc: 62.43%                   \n",
      "Val Loss:   0.9380 | Val Acc:   61.11%\n",
      "‚úì Migliorato! New best: 0.9380\n",
      "\n",
      "Epoch 9/100\n",
      "Train Loss: 0.9266 | Train Acc: 66.39%                   \n",
      "Val Loss:   1.0820 | Val Acc:   48.61%\n",
      "‚ö†Ô∏è Nessun miglioramento (1/10)\n",
      "\n",
      "Epoch 10/100\n",
      "Train Loss: 0.9296 | Train Acc: 67.08%                   \n",
      "Val Loss:   0.9932 | Val Acc:   68.06%\n",
      ">>> Model Saved!\n",
      "‚ö†Ô∏è Nessun miglioramento (2/10)\n",
      "\n",
      "Epoch 11/100\n",
      "Train Loss: 0.9080 | Train Acc: 68.96%                   \n",
      "Val Loss:   1.0368 | Val Acc:   61.11%\n",
      "‚ö†Ô∏è Nessun miglioramento (3/10)\n",
      "\n",
      "Epoch 12/100\n",
      "Train Loss: 0.9058 | Train Acc: 67.36%                   \n",
      "Val Loss:   1.0975 | Val Acc:   50.00%\n",
      "‚ö†Ô∏è Nessun miglioramento (4/10)\n",
      "\n",
      "Epoch 13/100\n",
      "Train Loss: 0.8348 | Train Acc: 72.92%                   \n",
      "Val Loss:   0.8055 | Val Acc:   77.78%\n",
      ">>> Model Saved!\n",
      "‚úì Migliorato! New best: 0.8055\n",
      "\n",
      "Epoch 14/100\n",
      "Train Loss: 0.8171 | Train Acc: 74.10%                   \n",
      "Val Loss:   0.8514 | Val Acc:   65.28%\n",
      "‚ö†Ô∏è Nessun miglioramento (1/10)\n",
      "\n",
      "Epoch 15/100\n",
      "Train Loss: 0.7886 | Train Acc: 76.46%                   \n",
      "Val Loss:   0.7768 | Val Acc:   77.78%\n",
      "‚úì Migliorato! New best: 0.7768\n",
      "\n",
      "Epoch 16/100\n",
      "Train Loss: 0.7482 | Train Acc: 78.82%                   \n",
      "Val Loss:   0.8847 | Val Acc:   63.89%\n",
      "‚ö†Ô∏è Nessun miglioramento (1/10)\n",
      "\n",
      "Epoch 17/100\n",
      "Train Loss: 0.7395 | Train Acc: 78.82%                   \n",
      "Val Loss:   1.1737 | Val Acc:   59.72%\n",
      "‚ö†Ô∏è Nessun miglioramento (2/10)\n",
      "\n",
      "Epoch 18/100\n",
      "Train Loss: 0.7226 | Train Acc: 79.58%                   \n",
      "Val Loss:   1.0300 | Val Acc:   65.28%\n",
      "‚ö†Ô∏è Nessun miglioramento (3/10)\n",
      "\n",
      "Epoch 19/100\n",
      "Train Loss: 0.6934 | Train Acc: 80.83%                   \n",
      "Val Loss:   0.9806 | Val Acc:   72.22%\n",
      "‚ö†Ô∏è Nessun miglioramento (4/10)\n",
      "\n",
      "Epoch 20/100\n",
      "Train Loss: 0.6559 | Train Acc: 83.12%                   \n",
      "Val Loss:   0.7619 | Val Acc:   76.39%\n",
      "‚úì Migliorato! New best: 0.7619\n",
      "\n",
      "Epoch 21/100\n",
      "Train Loss: 0.6288 | Train Acc: 85.21%                   \n",
      "Val Loss:   0.9001 | Val Acc:   70.83%\n",
      "‚ö†Ô∏è Nessun miglioramento (1/10)\n",
      "\n",
      "Epoch 22/100\n",
      "Train Loss: 0.6298 | Train Acc: 85.00%                   \n",
      "Val Loss:   0.7342 | Val Acc:   79.17%\n",
      ">>> Model Saved!\n",
      "‚úì Migliorato! New best: 0.7342\n",
      "\n",
      "Epoch 23/100\n",
      "Train Loss: 0.6471 | Train Acc: 83.61%                   \n",
      "Val Loss:   0.8242 | Val Acc:   68.06%\n",
      "‚ö†Ô∏è Nessun miglioramento (1/10)\n",
      "\n",
      "Epoch 24/100\n",
      "Train Loss: 0.5817 | Train Acc: 88.19%                   \n",
      "Val Loss:   0.8548 | Val Acc:   73.61%\n",
      "‚ö†Ô∏è Nessun miglioramento (2/10)\n",
      "\n",
      "Epoch 25/100\n",
      "Train Loss: 0.5932 | Train Acc: 88.40%                   \n",
      "Val Loss:   0.8736 | Val Acc:   75.00%\n",
      "‚ö†Ô∏è Nessun miglioramento (3/10)\n",
      "\n",
      "Epoch 26/100\n",
      "Train Loss: 0.5807 | Train Acc: 87.99%                   \n",
      "Val Loss:   0.8232 | Val Acc:   69.44%\n",
      "‚ö†Ô∏è Nessun miglioramento (4/10)\n",
      "\n",
      "Epoch 27/100\n",
      "Train Loss: 0.5399 | Train Acc: 90.49%                   \n",
      "Val Loss:   0.7366 | Val Acc:   80.56%\n",
      ">>> Model Saved!\n",
      "‚ö†Ô∏è Nessun miglioramento (5/10)\n",
      "\n",
      "Epoch 28/100\n",
      "Train Loss: 0.5338 | Train Acc: 91.25%                   \n",
      "Val Loss:   0.8173 | Val Acc:   79.17%\n",
      "‚ö†Ô∏è Nessun miglioramento (6/10)\n",
      "\n",
      "Epoch 29/100\n",
      "Train Loss: 0.5234 | Train Acc: 92.01%                   \n",
      "Val Loss:   0.8702 | Val Acc:   76.39%\n",
      "‚ö†Ô∏è Nessun miglioramento (7/10)\n",
      "\n",
      "Epoch 30/100\n",
      "Train Loss: 0.5207 | Train Acc: 92.57%                   \n",
      "Val Loss:   0.7533 | Val Acc:   81.94%\n",
      ">>> Model Saved!\n",
      "‚ö†Ô∏è Nessun miglioramento (8/10)\n",
      "\n",
      "Epoch 31/100\n",
      "Train Loss: 0.4943 | Train Acc: 94.24%                   \n",
      "Val Loss:   0.7930 | Val Acc:   81.94%\n",
      "‚ö†Ô∏è Nessun miglioramento (9/10)\n",
      "\n",
      "Epoch 32/100\n",
      "Train Loss: 0.4884 | Train Acc: 94.51%                   \n",
      "Val Loss:   0.8124 | Val Acc:   77.78%\n",
      "‚ö†Ô∏è Nessun miglioramento (10/10)\n",
      "üõë STOP! Nessun miglioramento per 10 epoche\n",
      "\n",
      "‚èπÔ∏è Early stopping attivato dopo 32 epoche\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Training Complete!\n",
      "Best Validation Accuracy: 81.94%\n",
      "================================================================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading history steps 31-31, summary, console lines 208-218 (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading history steps 31-31, summary, console lines 208-218 (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m uploading history steps 31-31, summary, console lines 208-218 (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ‚ñÅ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÅ‚ñÉ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñÉ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 94.51389\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.48842\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 77.77778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.81244\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mtrain_20260106_164902\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pagliarellomatteo-politecnico-di-torino/speech-emotion-recognition/runs/e4q1nboi\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pagliarellomatteo-politecnico-di-torino/speech-emotion-recognition\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20260106_154902-e4q1nboi/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DyQo3klIymlz"
   },
   "source": [
    "# Step 5: Evaluate your model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "9goKvp4jxk4j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Timestamp valutazione: 20260106_170828\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpagliarellomatteo\u001b[0m (\u001b[33mpagliarellomatteo-politecnico-di-torino\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/speech-emotion-recognition-25/speech-emotion-recognition-25/wandb/run-20260106_160829-4c9unxz0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meval_20260106_170828\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/pagliarellomatteo-politecnico-di-torino/speech-emotion-recognition\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pagliarellomatteo-politecnico-di-torino/speech-emotion-recognition/runs/4c9unxz0\u001b[0m\n",
      "‚úÖ RAVDESS trovato: /kaggle/input/ravdess-emotional-speech-audio\n",
      "\n",
      "Loading RAVDESS test set...\n",
      "üìä Statistiche del dataset RAVDESS:\n",
      "\n",
      "========================================\n",
      "üìä ANALISI RAVDESS TEST SET\n",
      "========================================\n",
      "üîπ Samples Totali: 144\n",
      "üîπ Attori (2): [23, 24]\n",
      "   - Maschi:  1\n",
      "   - Femmine: 1\n",
      "\n",
      "üé≠ Distribuzione Emozioni:\n",
      "   - Angry     :   32 ( 22.2%) ‚ñà‚ñà‚ñà‚ñà\n",
      "   - Happy     :   32 ( 22.2%) ‚ñà‚ñà‚ñà‚ñà\n",
      "   - Neutral   :   48 ( 33.3%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   - Sad       :   32 ( 22.2%) ‚ñà‚ñà‚ñà‚ñà\n",
      "----------------------------------------\n",
      "‚úÖ Test samples: 144\n",
      "\n",
      "Loading model...\n",
      "‚úÖ Modello caricato da checkpoints/best_model.pth\n",
      "\n",
      "================================================================================\n",
      "TESTING IN CORSO...\n",
      "================================================================================\n",
      "\n",
      "üìä Calcolando statistiche durata POST-TRIMMING per split 'test'...\n",
      "   28/144 file processati...\n",
      "   56/144 file processati...\n",
      "   84/144 file processati...\n",
      "   112/144 file processati...\n",
      "   140/144 file processati...\n",
      "‚úÖ Media: 2.04s (32583 campioni)\n",
      "‚úÖ Massimo: 3.65s (58368 campioni)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä METRICHE DI VALUTAZIONE - FASE 1 (RAVDESS BASELINE)\n",
      "================================================================================\n",
      "\n",
      "üéØ METRICHE PRINCIPALI:\n",
      "   ‚úÖ Accuracy:           63.89%\n",
      "   üìà Macro-Avg F1:       0.6349\n",
      "   üìä Weighted-Avg F1:    0.6368\n",
      "\n",
      "üìã CLASS DISTRIBUTION (Test Set):\n",
      "   neutral   :  48 samples ( 33.3%)\n",
      "   happy     :  32 samples ( 22.2%)\n",
      "   sad       :  32 samples ( 22.2%)\n",
      "   angry     :  32 samples ( 22.2%)\n",
      "\n",
      "üé≠ DETAILED CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.68      0.62      0.65        48\n",
      "       happy       0.48      0.94      0.64        32\n",
      "         sad       0.67      0.38      0.48        32\n",
      "       angry       1.00      0.62      0.77        32\n",
      "\n",
      "    accuracy                           0.64       144\n",
      "   macro avg       0.71      0.64      0.63       144\n",
      "weighted avg       0.71      0.64      0.64       144\n",
      "\n",
      "Figure(1000x800)\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Evaluation Complete!\n",
      "   Final Accuracy: 63.89%\n",
      "   Macro-Avg F1:   0.6349\n",
      "   Weighted-Avg F1: 0.6368\n",
      "   Risultati loggati su W&B\n",
      "================================================================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-4c9unxz0-classification_report (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run-4c9unxz0-classification_report (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run-4c9unxz0-classification_report (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run-4c9unxz0-classification_report (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading artifact run-4c9unxz0-classification_report (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ‚Ü≥ \u001b[38;5;178m‚£∑\u001b[0m classification_report.table.json 977B/977B (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading artifact run-4c9unxz0-classification_report (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ‚Ü≥ \u001b[38;5;178m‚£Ø\u001b[0m classification_report.table.json 977B/977B (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m uploading artifact run-4c9unxz0-classification_report (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ‚Ü≥ \u001b[38;5;178m‚£ü\u001b[0m classification_report.table.json 977B/977B (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚°ø\u001b[0m uploading artifact run-4c9unxz0-classification_report (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ‚Ü≥ \u001b[38;5;178m‚°ø\u001b[0m classification_report.table.json 977B/977B (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-4c9unxz0-classification_report (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ‚Ü≥ \u001b[38;5;178m‚¢ø\u001b[0m classification_report.table.json 977B/977B (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run-4c9unxz0-classification_report (1.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run-4c9unxz0-classification_report (1.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run-4c9unxz0-classification_report (1.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    test_accuracy ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    test_macro_f1 ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_weighted_f1 ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    test_accuracy 0.63889\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    test_macro_f1 0.63493\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_weighted_f1 0.63684\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33meval_20260106_170828\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pagliarellomatteo-politecnico-di-torino/speech-emotion-recognition/runs/4c9unxz0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pagliarellomatteo-politecnico-di-torino/speech-emotion-recognition\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20260106_160829-4c9unxz0/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python eval.py"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
