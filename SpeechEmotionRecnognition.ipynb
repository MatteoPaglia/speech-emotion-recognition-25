{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-E9qeg-6y1Hu"
      },
      "source": [
        "# LAB 3: How to setup a project from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHmVt4s034WK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6kJGhxzyN6d"
      },
      "source": [
        "# Step 1: Clone your project from Github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Wfm084txMr0",
        "outputId": "91f0ad75-ff73-4cfb-b5f1-be42a8c96886"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'speech-emotion-recognition-25'...\n",
            "remote: Enumerating objects: 78, done.\u001b[K\n",
            "remote: Counting objects: 100% (67/67), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 78 (delta 32), reused 44 (delta 11), pack-reused 11 (from 2)\u001b[K\n",
            "Receiving objects: 100% (78/78), 20.18 KiB | 10.09 MiB/s, done.\n",
            "Resolving deltas: 100% (32/32), done.\n"
          ]
        }
      ],
      "source": [
        "#main\n",
        "#!git clone https://github.com/MatteoPaglia/speech-emotion-recognition-25.git\n",
        "\n",
        "#             nome branch\n",
        "!rm -rf speech-emotion-recognition-25\n",
        "!git clone -b DatasetSetup https://github.com/MatteoPaglia/speech-emotion-recognition-25.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MN0lMneJxVz0",
        "outputId": "64b85f65-94db-4b58-9b9b-1e41effc2001"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "checkpoints  eval.py\trequirements.txt\t\t train.py\n",
            "data\t     models\tSpeechEmotionRecnognition.ipynb  utils\n",
            "dataset      README.md\tspeech-emotion-recognition-25\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Pa5nOPxxbDf",
        "outputId": "cb19573e-9b7e-42c0-abe3-92b4fc6493de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/speech-emotion-recognition-25/speech-emotion-recognition-25\n"
          ]
        }
      ],
      "source": [
        "# %cd mldl_project_skeleton\n",
        "%cd speech-emotion-recognition-25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYilllpZzKMz",
        "outputId": "21364bda-49b5-450d-b782-846928e1bdd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "checkpoints  dataset  models\t requirements.txt\t\t  train.py\n",
            "data\t     eval.py  README.md  SpeechEmotionRecnognition.ipynb  utils\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be_4yDyp1Hru"
      },
      "source": [
        "# Step 2: Packages Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "EO9DuAYk1LFR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (1.7.4.5)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (0.3.13)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (2025.11.12)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub->-r requirements.txt (line 2)) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub->-r requirements.txt (line 2)) (6.0.3)\n"
          ]
        }
      ],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bifSi62Ixrqr"
      },
      "source": [
        "# Step 3: Dataset Setup\n",
        "## Different options\n",
        "- First one is downloading using a script that places the data in the download folder (usually recommended)\n",
        "- Second one is uploading the dataset to your personal/institutional Google Drive and load it from there ([Read More](https://saturncloud.io/blog/google-colab-how-to-read-data-from-my-google-drive/))\n",
        "- Place the download script directly here on colab\n",
        "\n",
        "You are free to do as you please in this phase.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DiWQTaTbxeIc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "DOWNLOAD AUTOMATICO DATASET\n",
            "============================================================\n",
            "--- 1. Configurazione Kaggle ---\n",
            "‚úÖ Kaggle configurato con successo.\n",
            "\n",
            "============================================================\n",
            "\n",
            "--- Download RAVDESS ---\n",
            "Contatto KaggleHub per scaricare: uwrfkaggler/ravdess-emotional-speech-audio...\n",
            "Using Colab cache for faster access to the 'ravdess-emotional-speech-audio' dataset.\n",
            "‚úì Dataset scaricato nella cache di sistema: /kaggle/input/ravdess-emotional-speech-audio\n",
            "La cartella locale './ravdess' esiste gi√†. La rimuovo per aggiornarla...\n",
            "Copia dei file nella cartella di lavoro: ./ravdess...\n",
            "‚úÖ RAVDESS pronto in: ./ravdess\n",
            "‚úÖ Numero totale di file copiati: 2880\n",
            "\n",
            "‚ö†Ô∏è  NOTA: Sto usando il dataset 'dejolilandry/iemocapfullrelease'\n",
            "    che √® verificato funzionante e non d√† errore 403.\n",
            "\n",
            "\n",
            "--- Download IEMOCAP ---\n",
            "Contatto KaggleHub per scaricare: dejolilandry/iemocapfullrelease...\n",
            "‚úì Dataset scaricato nella cache di sistema: /root/.cache/kagglehub/datasets/dejolilandry/iemocapfullrelease/versions/1\n",
            "La cartella locale './iemocap' esiste gi√†. La rimuovo per aggiornarla...\n",
            "Copia dei file nella cartella di lavoro: ./iemocap...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/speech-emotion-recognition-25/speech-emotion-recognition-25/utils/download_dataset.py\", line 107, in <module>\n",
            "    iemocap_ok = download_iemocap()\n",
            "                 ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/speech-emotion-recognition-25/speech-emotion-recognition-25/utils/download_dataset.py\", line 91, in download_iemocap\n",
            "    return download_dataset_via_hub(\"dejolilandry/iemocapfullrelease\", \"iemocap\")\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/speech-emotion-recognition-25/speech-emotion-recognition-25/utils/download_dataset.py\", line 65, in download_dataset_via_hub\n",
            "    shutil.copytree(cached_path, destination_dir)\n",
            "  File \"/usr/lib/python3.12/shutil.py\", line 600, in copytree\n",
            "    return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/shutil.py\", line 536, in _copytree\n",
            "    copytree(srcobj, dstname, symlinks, ignore, copy_function,\n",
            "  File \"/usr/lib/python3.12/shutil.py\", line 600, in copytree\n",
            "    return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/shutil.py\", line 536, in _copytree\n",
            "    copytree(srcobj, dstname, symlinks, ignore, copy_function,\n",
            "  File \"/usr/lib/python3.12/shutil.py\", line 600, in copytree\n",
            "  File \"/usr/lib/python3.12/shutil.py\", line 536, in _copytree\n",
            "    copytree(srcobj, dstname, symlinks, ignore, copy_function,\n",
            "  File \"/usr/lib/python3.12/shutil.py\", line 600, in copytree\n",
            "    return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/shutil.py\", line 536, in _copytree\n",
            "    copytree(srcobj, dstname, symlinks, ignore, copy_function,\n",
            "  File \"/usr/lib/python3.12/shutil.py\", line 600, in copytree\n",
            "    return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/shutil.py\", line 536, in _copytree\n",
            "    copytree(srcobj, dstname, symlinks, ignore, copy_function,\n",
            "  File \"/usr/lib/python3.12/shutil.py\", line 600, in copytree\n",
            "    return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/shutil.py\", line 540, in _copytree\n",
            "    copy_function(srcobj, dstname)\n",
            "  File \"/usr/lib/python3.12/shutil.py\", line 475, in copy2\n",
            "    copyfile(src, dst, follow_symlinks=follow_symlinks)\n",
            "  File \"/usr/lib/python3.12/shutil.py\", line 273, in copyfile\n",
            "    _fastcopy_sendfile(fsrc, fdst)\n",
            "  File \"/usr/lib/python3.12/shutil.py\", line 150, in _fastcopy_sendfile\n",
            "    sent = os.sendfile(outfd, infd, offset, blocksize)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python utils/download_dataset.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ESPLORAZIONE STRUTTURA DATASET\n",
        "# ============================================================================\n",
        "\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "def analyze_dataset_structure(dataset_path, dataset_name):\n",
        "    \"\"\"Analizza e mostra la struttura di un dataset\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"üìÅ STRUTTURA DATASET: {dataset_name.upper()}\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    path = Path(dataset_path)\n",
        "    \n",
        "    if not path.exists():\n",
        "        print(f\"‚ùå Directory non trovata: {dataset_path}\")\n",
        "        return\n",
        "    \n",
        "    # 1. STATISTICHE GENERALI\n",
        "    print(\"\\nüìä STATISTICHE GENERALI:\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    total_files = 0\n",
        "    total_size = 0\n",
        "    file_extensions = defaultdict(int)\n",
        "    \n",
        "    for item in path.rglob('*'):\n",
        "        if item.is_file():\n",
        "            total_files += 1\n",
        "            total_size += item.stat().st_size\n",
        "            ext = item.suffix.lower() or '[no extension]'\n",
        "            file_extensions[ext] += 1\n",
        "    \n",
        "    print(f\"  ‚Ä¢ Totale file: {total_files}\")\n",
        "    print(f\"  ‚Ä¢ Dimensione totale: {total_size / (1024**3):.2f} GB\")\n",
        "    print(f\"  ‚Ä¢ Tipi di file:\")\n",
        "    for ext, count in sorted(file_extensions.items(), key=lambda x: x[1], reverse=True):\n",
        "        print(f\"      {ext}: {count} file(s)\")\n",
        "    \n",
        "    # 2. STRUTTURA DIRECTORY (primi 3 livelli)\n",
        "    print(\"\\nüìÇ STRUTTURA DIRECTORY (primi 3 livelli):\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    def print_tree(directory, prefix=\"\", max_depth=3, current_depth=0):\n",
        "        if current_depth >= max_depth:\n",
        "            return\n",
        "        \n",
        "        try:\n",
        "            items = sorted(directory.iterdir(), key=lambda x: (not x.is_dir(), x.name))\n",
        "            dirs = [item for item in items if item.is_dir()]\n",
        "            files = [item for item in items if item.is_file()]\n",
        "            \n",
        "            # Mostra prime 3 directory\n",
        "            for i, item in enumerate(dirs[:3]):\n",
        "                is_last = (i == len(dirs[:3]) - 1) and len(files) == 0\n",
        "                print(f\"{prefix}{'‚îî‚îÄ‚îÄ ' if is_last else '‚îú‚îÄ‚îÄ '}üìÅ {item.name}/\")\n",
        "                \n",
        "                extension = \"    \" if is_last else \"‚îÇ   \"\n",
        "                print_tree(item, prefix + extension, max_depth, current_depth + 1)\n",
        "            \n",
        "            if len(dirs) > 3:\n",
        "                print(f\"{prefix}‚îî‚îÄ‚îÄ ... altre {len(dirs) - 3} cartelle\")\n",
        "            \n",
        "            # Mostra primi 3 file\n",
        "            if files and current_depth < max_depth - 1:\n",
        "                for i, item in enumerate(files[:3]):\n",
        "                    is_last = i == len(files[:3]) - 1\n",
        "                    size_mb = item.stat().st_size / (1024**2)\n",
        "                    print(f\"{prefix}{'‚îî‚îÄ‚îÄ ' if is_last else '‚îú‚îÄ‚îÄ '}üìÑ {item.name} ({size_mb:.2f} MB)\")\n",
        "                \n",
        "                if len(files) > 3:\n",
        "                    print(f\"{prefix}‚îî‚îÄ‚îÄ ... altri {len(files) - 3} file\")\n",
        "        \n",
        "        except PermissionError:\n",
        "            print(f\"{prefix}‚ùå Accesso negato\")\n",
        "    \n",
        "    print_tree(path)\n",
        "    \n",
        "    # 3. ESEMPIO FILE PATHS (primi 5)\n",
        "    print(\"\\nüìù ESEMPIO PERCORSI FILE (primi 5):\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    all_files = list(path.rglob('*'))\n",
        "    file_list = [f for f in all_files if f.is_file()][:5]\n",
        "    \n",
        "    for f in file_list:\n",
        "        relative_path = f.relative_to(path)\n",
        "        size_mb = f.stat().st_size / (1024**2)\n",
        "        print(f\"  ‚Ä¢ {relative_path}\")\n",
        "        print(f\"    Dimensione: {size_mb:.2f} MB\")\n",
        "    \n",
        "    # 4. DISTRIBUZIONE PER LIVELLO\n",
        "    print(\"\\nüìä DISTRIBUZIONE FILE PER LIVELLO:\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    level_distribution = defaultdict(int)\n",
        "    for item in path.rglob('*'):\n",
        "        if item.is_file():\n",
        "            depth = len(item.relative_to(path).parts) - 1\n",
        "            level_distribution[depth] += 1\n",
        "    \n",
        "    for level in sorted(level_distribution.keys()):\n",
        "        print(f\"  Livello {level}: {level_distribution[level]} file(s)\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# ANALISI RAVDESS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"üéµ\" * 40)\n",
        "print(\"ANALISI DATASET RAVDESS\")\n",
        "print(\"üéµ\" * 40 + \"\\n\")\n",
        "\n",
        "analyze_dataset_structure(\"./ravdess\", \"RAVDESS\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# ANALISI IEMOCAP\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"üéôÔ∏è\" * 40)\n",
        "print(\"ANALISI DATASET IEMOCAP\")\n",
        "print(\"üéôÔ∏è\" * 40 + \"\\n\")\n",
        "\n",
        "analyze_dataset_structure(\"./iemocap\", \"IEMOCAP\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqo9Eh79yihI"
      },
      "source": [
        "# Step 4: Train your model and visualize training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-dxDQOFcdgX"
      },
      "outputs": [],
      "source": [
        "#%env WANDB_API_KEY=\"7ade30086de7899bed412e3eb5c2da065c146f90\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8q9OvEDHxmRv"
      },
      "outputs": [],
      "source": [
        "#!python train.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyQo3klIymlz"
      },
      "source": [
        "# Step 5: Evaluate your model\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9goKvp4jxk4j"
      },
      "outputs": [],
      "source": [
        "#!python eval.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
