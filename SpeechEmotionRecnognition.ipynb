{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-E9qeg-6y1Hu"
   },
   "source": [
    "# LAB 3: How to setup a project from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XHmVt4s034WK"
   },
   "outputs": [],
   "source": [
    "!rm -rf speech-emotion-recognition-25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6kJGhxzyN6d"
   },
   "source": [
    "# Step 1: Clone your project from Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Wfm084txMr0",
    "outputId": "91f0ad75-ff73-4cfb-b5f1-be42a8c96886"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'speech-emotion-recognition-25'...\n",
      "remote: Enumerating objects: 584, done.\u001b[K\n",
      "remote: Counting objects: 100% (155/155), done.\u001b[K\n",
      "remote: Compressing objects: 100% (109/109), done.\u001b[K\n",
      "remote: Total 584 (delta 100), reused 94 (delta 46), pack-reused 429 (from 3)\u001b[K\n",
      "Receiving objects: 100% (584/584), 3.26 MiB | 29.02 MiB/s, done.\n",
      "Resolving deltas: 100% (360/360), done.\n"
     ]
    }
   ],
   "source": [
    "#main\n",
    "#!git clone https://github.com/MatteoPaglia/speech-emotion-recognition-25.git\n",
    "\n",
    "#             nome branch\n",
    "\n",
    "!git clone -b RavdnessTrain https://github.com/MatteoPaglia/speech-emotion-recognition-25.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MN0lMneJxVz0",
    "outputId": "64b85f65-94db-4b58-9b9b-1e41effc2001"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data  speech-emotion-recognition-25\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Pa5nOPxxbDf",
    "outputId": "cb19573e-9b7e-42c0-abe3-92b4fc6493de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/speech-emotion-recognition-25\n"
     ]
    }
   ],
   "source": [
    "# %cd mldl_project_skeleton\n",
    "%cd speech-emotion-recognition-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VYilllpZzKMz",
    "outputId": "21364bda-49b5-450d-b782-846928e1bdd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints  dataset  README.md\t\t\t       train.py\n",
      "config.py    eval.py  requirements.txt\t\t       utils\n",
      "data\t     models   SpeechEmotionRecnognition.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be_4yDyp1Hru"
   },
   "source": [
    "# Step 2: Packages Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EO9DuAYk1LFR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (1.7.4.5)\n",
      "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (0.3.13)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (1.6.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (2.9.0+cu126)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (2.9.0+cu126)\n",
      "Collecting torchcodec (from -r requirements.txt (line 6))\n",
      "  Downloading torchcodec-0.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (0.24.0+cu126)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (0.11.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (2.0.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (3.10.0)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (0.13.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (4.67.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 13)) (11.3.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 14)) (6.0.3)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 15)) (0.23.1)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (6.3.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (2026.1.4)\n",
      "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (3.4.4)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (3.11)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (5.29.5)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (8.0.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (75.2.0)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (1.3)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirements.txt (line 1)) (0.5.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub->-r requirements.txt (line 2)) (25.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 3)) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 3)) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 3)) (3.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (3.20.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (3.5.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 8)) (3.1.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 8)) (0.60.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 8)) (4.4.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 8)) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 8)) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 8)) (1.0.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 8)) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 8)) (1.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 10)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 10)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 10)) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 10)) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 10)) (3.3.1)\n",
      "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn->-r requirements.txt (line 11)) (2.2.2)\n",
      "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 15)) (8.3.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 15)) (3.1.46)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 15)) (4.5.1)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 15)) (2.12.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 15)) (2.49.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 15)) (4.0.12)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa->-r requirements.txt (line 8)) (0.43.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn->-r requirements.txt (line 11)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn->-r requirements.txt (line 11)) (2025.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 15)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 15)) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 15)) (0.4.2)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa->-r requirements.txt (line 8)) (2.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r requirements.txt (line 4)) (3.0.3)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->-r requirements.txt (line 8)) (2.23)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 15)) (5.0.2)\n",
      "Downloading torchcodec-0.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchcodec\n",
      "Successfully installed torchcodec-0.9.1\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bifSi62Ixrqr"
   },
   "source": [
    "# Step 3: Dataset Setup\n",
    "## Different options\n",
    "- First one is downloading using a script that places the data in the download folder (usually recommended)\n",
    "- Second one is uploading the dataset to your personal/institutional Google Drive and load it from there ([Read More](https://saturncloud.io/blog/google-colab-how-to-read-data-from-my-google-drive/))\n",
    "- Place the download script directly here on colab\n",
    "\n",
    "You are free to do as you please in this phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DiWQTaTbxeIc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Configurazione Kaggle ---\n",
      "Kaggle configurato con successo.\n",
      "\n",
      "--- Download RAVDESS ---\n",
      "Contatto KaggleHub per scaricare: uwrfkaggler/ravdess-emotional-speech-audio...\n",
      "Using Colab cache for faster access to the 'ravdess-emotional-speech-audio' dataset.\n",
      "âœ“ Dataset scaricato nella cache di sistema: /kaggle/input/ravdess-emotional-speech-audio\n",
      "RAVDESS pronto in cache: /kaggle/input/ravdess-emotional-speech-audio\n",
      "Numero totale di file: 2880\n",
      "\n",
      "--- Download IEMOCAP ---\n",
      "Contatto KaggleHub per scaricare: dejolilandry/iemocapfullrelease...\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.0).\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/dejolilandry/iemocapfullrelease?dataset_version_number=1...\n",
      "100% 11.5G/11.5G [02:10<00:00, 94.6MB/s]\n",
      "Extracting files...\n",
      "âœ“ Dataset scaricato nella cache di sistema: /root/.cache/kagglehub/datasets/dejolilandry/iemocapfullrelease/versions/1\n",
      "IEMOCAP pronto in cache: /root/.cache/kagglehub/datasets/dejolilandry/iemocapfullrelease/versions/1\n",
      "Numero totale di file: 81249\n",
      "\n",
      "============================================================\n",
      "RIEPILOGO DOWNLOAD\n",
      "============================================================\n",
      "RAVDESS: âœ… Successo\n",
      "IEMOCAP: âœ… Successo\n",
      "============================================================\n",
      "\n",
      "ğŸ‰ Tutti i dataset sono stati scaricati con successo!\n"
     ]
    }
   ],
   "source": [
    "!python utils/download_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Path diretti\n",
    "iemocap_path = Path('/kaggle/input/iemocapfullrelease/IEMOCAP_full_release')\n",
    "ravdess_path = Path('/kaggle/input/ravdess-emotional-speech-audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n\\nimport os\\nfrom pathlib import Path\\n\\nprint(\"=\"*80)\\nprint(\"ğŸ” RICERCA PERCORSI DATASET\")\\nprint(\"=\"*80)\\n\\n# Percorsi possibili dove potrebbero essere i dataset\\npossible_paths = [\\n    Path(\\'/kaggle/input/\\'),\\n    Path.home() / \\'.cache\\' / \\'kagglehub\\' / \\'datasets\\',\\n    Path(\\'/root/.cache/kagglehub/datasets\\'),\\n    Path(\\'/tmp/kagglehub/datasets\\'),\\n    Path(\\'./data\\'),\\n    Path(\\'../data\\'),\\n    Path(\\'../../data\\'),\\n]\\n\\n# Aggiungi anche la directory corrente\\npossible_paths.append(Path.cwd())\\n\\nprint(f\"\\nğŸ“ Directory corrente: {Path.cwd()}\\n\")\\n\\n# Ricerca IEMOCAP\\nprint(\"ğŸ” Ricerca IEMOCAP_full_release...\")\\niemocap_path = None\\niemocap_found = False\\nfor base_path in possible_paths:\\n    if base_path.exists():\\n        for root, dirs, files in os.walk(base_path):\\n            if \\'IEMOCAP_full_release\\' in dirs:\\n                iemocap_path = Path(root) / \\'IEMOCAP_full_release\\'\\n                print(f\"âœ… IEMOCAP trovato a: {iemocap_path}\")\\n                iemocap_found = True\\n                break\\n    if iemocap_found:\\n        break\\n\\nif not iemocap_found:\\n    print(\"âŒ IEMOCAP non trovato nei percorsi standard\")\\n    iemocap_path = None\\n\\n# Ricerca RAVDESS\\nprint(\"\\nğŸ” Ricerca ravdess-emotional-speech-audio...\")\\nravdess_path = None\\nravdess_found = False\\nfor base_path in possible_paths:\\n    if base_path.exists():\\n        for root, dirs, files in os.walk(base_path):\\n            if \\'ravdess-emotional-speech-audio\\' in dirs:\\n                ravdess_path = Path(root) / \\'ravdess-emotional-speech-audio\\'\\n                print(f\"âœ… RAVDESS trovato a: {ravdess_path}\")\\n                ravdess_found = True\\n                break\\n    if ravdess_found:\\n        break\\n\\nif not ravdess_found:\\n    print(\"âŒ RAVDESS non trovato nei percorsi standard\")\\n    ravdess_path = None\\n\\n# Lista contenuti della directory data/ se esiste\\nprint(\"\\nğŸ“‚ Contenuto della cartella \\'data/\\' (se presente):\")\\ndata_dir = Path(\\'./data\\')\\nif data_dir.exists():\\n    for item in data_dir.iterdir():\\n        print(f\"   - {item.name}\")\\nelse:\\n    print(\"   âŒ Cartella \\'data/\\' non trovata\")\\n\\nprint(\"\\n\" + \"=\"*80)\\nprint(\"âœ… VARIABILI SALVATE:\")\\nprint(f\"   - iemocap_path = {iemocap_path}\")\\nprint(f\"   - ravdess_path = {ravdess_path}\")\\nprint(\"=\"*80)\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cerca e stampa i percorsi dei dataset\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ” RICERCA PERCORSI DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Percorsi possibili dove potrebbero essere i dataset\n",
    "possible_paths = [\n",
    "    Path('/kaggle/input/'),\n",
    "    Path.home() / '.cache' / 'kagglehub' / 'datasets',\n",
    "    Path('/root/.cache/kagglehub/datasets'),\n",
    "    Path('/tmp/kagglehub/datasets'),\n",
    "    Path('./data'),\n",
    "    Path('../data'),\n",
    "    Path('../../data'),\n",
    "]\n",
    "\n",
    "# Aggiungi anche la directory corrente\n",
    "possible_paths.append(Path.cwd())\n",
    "\n",
    "print(f\"\\nğŸ“ Directory corrente: {Path.cwd()}\\n\")\n",
    "\n",
    "# Ricerca IEMOCAP\n",
    "print(\"ğŸ” Ricerca IEMOCAP_full_release...\")\n",
    "iemocap_path = None\n",
    "iemocap_found = False\n",
    "for base_path in possible_paths:\n",
    "    if base_path.exists():\n",
    "        for root, dirs, files in os.walk(base_path):\n",
    "            if 'IEMOCAP_full_release' in dirs:\n",
    "                iemocap_path = Path(root) / 'IEMOCAP_full_release'\n",
    "                print(f\"âœ… IEMOCAP trovato a: {iemocap_path}\")\n",
    "                iemocap_found = True\n",
    "                break\n",
    "    if iemocap_found:\n",
    "        break\n",
    "\n",
    "if not iemocap_found:\n",
    "    print(\"âŒ IEMOCAP non trovato nei percorsi standard\")\n",
    "    iemocap_path = None\n",
    "\n",
    "# Ricerca RAVDESS\n",
    "print(\"\\nğŸ” Ricerca ravdess-emotional-speech-audio...\")\n",
    "ravdess_path = None\n",
    "ravdess_found = False\n",
    "for base_path in possible_paths:\n",
    "    if base_path.exists():\n",
    "        for root, dirs, files in os.walk(base_path):\n",
    "            if 'ravdess-emotional-speech-audio' in dirs:\n",
    "                ravdess_path = Path(root) / 'ravdess-emotional-speech-audio'\n",
    "                print(f\"âœ… RAVDESS trovato a: {ravdess_path}\")\n",
    "                ravdess_found = True\n",
    "                break\n",
    "    if ravdess_found:\n",
    "        break\n",
    "\n",
    "if not ravdess_found:\n",
    "    print(\"âŒ RAVDESS non trovato nei percorsi standard\")\n",
    "    ravdess_path = None\n",
    "\n",
    "# Lista contenuti della directory data/ se esiste\n",
    "print(\"\\nğŸ“‚ Contenuto della cartella 'data/' (se presente):\")\n",
    "data_dir = Path('./data')\n",
    "if data_dir.exists():\n",
    "    for item in data_dir.iterdir():\n",
    "        print(f\"   - {item.name}\")\n",
    "else:\n",
    "    print(\"   âŒ Cartella 'data/' non trovata\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… VARIABILI SALVATE:\")\n",
    "print(f\"   - iemocap_path = {iemocap_path}\")\n",
    "print(f\"   - ravdess_path = {ravdess_path}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' \\n\\nfrom torch.utils.data import DataLoader\\nfrom dataset.custom_iemocap_dataset import CustomIEMOCAPDataset\\nfrom dataset.custom_ravdess_dataset import CustomRAVDESSDataset\\n\\nprint(f\"////////////////////////////////////////////////////////////////////////////////////////////\")\\nprint(f\"Dataset IEMOCAP\")\\nprint(f\"////////////////////////////////////////////////////////////////////////////////////////////\")\\n\\n# Usa il percorso trovato in precedenza, altrimenti fallback\\nif iemocap_path and iemocap_path.exists():\\n    dataset_IEMOCAP_path = str(iemocap_path)\\n    print(f\"âœ… Usando percorso trovato: {dataset_IEMOCAP_path}\")\\nelse:\\n    dataset_IEMOCAP_path = \\'/kaggle/input/iemocapfullrelease/IEMOCAP_full_release\\'\\n    print(f\"âš ï¸  Percorso non trovato, usando fallback: {dataset_IEMOCAP_path}\")\\n\\n# Create IEMOCAPdatasets\\ntrain_IEMOCAP_dataset = CustomIEMOCAPDataset(dataset_root=dataset_IEMOCAP_path, split=\\'train\\')\\nval_IEMOCAP_dataset = CustomIEMOCAPDataset(dataset_root=dataset_IEMOCAP_path, split=\\'validation\\')\\ntest_IEMOCAP_dataset = CustomIEMOCAPDataset(dataset_root=dataset_IEMOCAP_path, split=\\'test\\')\\n\\nprint(f\"Train samples: {len(train_IEMOCAP_dataset)}\")\\nprint(f\"Val samples: {len(val_IEMOCAP_dataset)}\")\\nprint(f\"Test samples: {len(test_IEMOCAP_dataset)}\")\\n\\n# Create IEMOCAP DataLoaders\\nbatch_size = 4\\ntrain_IEMOCAP_dataloader = DataLoader(train_IEMOCAP_dataset, batch_size=batch_size, shuffle=True)\\nval_IEMOCAP_dataloader = DataLoader(val_IEMOCAP_dataset, batch_size=batch_size, shuffle=False)\\ntest_IEMOCAP_dataloader = DataLoader(test_IEMOCAP_dataset, batch_size=batch_size, shuffle=False)\\n\\n\\nprint(f\"////////////////////////////////////////////////////////////////////////////////////////////\")\\nprint(f\"Dataset RAVDESS\")\\nprint(f\"////////////////////////////////////////////////////////////////////////////////////////////\")\\n\\n# Usa il percorso trovato in precedenza, altrimenti fallback\\nif ravdess_path and ravdess_path.exists():\\n    dataset_RAVDESS_path = str(ravdess_path)\\n    print(f\"âœ… Usando percorso trovato: {dataset_RAVDESS_path}\")\\nelse:\\n    dataset_RAVDESS_path = \\'/kaggle/input/ravdess-emotional-speech-audio\\'\\n    print(f\"âš ï¸  Percorso non trovato, usando fallback: {dataset_RAVDESS_path}\")\\n\\n# Create RAVDESS datasets\\ntrain_RAVDESS_dataset = CustomRAVDESSDataset(dataset_root=dataset_RAVDESS_path, split=\\'train\\')\\nval_RAVDESS_dataset = CustomRAVDESSDataset(dataset_root=dataset_RAVDESS_path, split=\\'validation\\')\\ntest_RAVDESS_dataset = CustomRAVDESSDataset(dataset_root=dataset_RAVDESS_path, split=\\'test\\')\\n\\nprint(f\"Train samples: {len(train_RAVDESS_dataset)}\")\\nprint(f\"Val samples: {len(val_RAVDESS_dataset)}\")\\nprint(f\"Test samples: {len(test_RAVDESS_dataset)}\")\\n\\n# Create RAVDESS DataLoaders\\nbatch_size = 4\\ntrain_RAVDESS_dataloader = DataLoader(train_RAVDESS_dataset, batch_size=batch_size, shuffle=True)\\nval_RAVDESS_dataloader = DataLoader(val_RAVDESS_dataset, batch_size=batch_size, shuffle=False)\\ntest_RAVDESS_dataloader = DataLoader(test_RAVDESS_dataset, batch_size=batch_size, shuffle=False)\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crea dei DataLoader per stampare le statisiche \n",
    "\n",
    "#TODO:  #creare un file per stampare le statistiche utili e toglierle dalle classi\n",
    "# Ã¨ necessario creare dei DataLoader per poter calcolare le statistiche sui dataset? \n",
    "\n",
    "\"\"\" \n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset.custom_iemocap_dataset import CustomIEMOCAPDataset\n",
    "from dataset.custom_ravdess_dataset import CustomRAVDESSDataset\n",
    "\n",
    "print(f\"////////////////////////////////////////////////////////////////////////////////////////////\")\n",
    "print(f\"Dataset IEMOCAP\")\n",
    "print(f\"////////////////////////////////////////////////////////////////////////////////////////////\")\n",
    "\n",
    "# Usa il percorso trovato in precedenza, altrimenti fallback\n",
    "if iemocap_path and iemocap_path.exists():\n",
    "    dataset_IEMOCAP_path = str(iemocap_path)\n",
    "    print(f\"âœ… Usando percorso trovato: {dataset_IEMOCAP_path}\")\n",
    "else:\n",
    "    dataset_IEMOCAP_path = '/kaggle/input/iemocapfullrelease/IEMOCAP_full_release'\n",
    "    print(f\"âš ï¸  Percorso non trovato, usando fallback: {dataset_IEMOCAP_path}\")\n",
    "\n",
    "# Create IEMOCAPdatasets\n",
    "train_IEMOCAP_dataset = CustomIEMOCAPDataset(dataset_root=dataset_IEMOCAP_path, split='train')\n",
    "val_IEMOCAP_dataset = CustomIEMOCAPDataset(dataset_root=dataset_IEMOCAP_path, split='validation')\n",
    "test_IEMOCAP_dataset = CustomIEMOCAPDataset(dataset_root=dataset_IEMOCAP_path, split='test')\n",
    "\n",
    "print(f\"Train samples: {len(train_IEMOCAP_dataset)}\")\n",
    "print(f\"Val samples: {len(val_IEMOCAP_dataset)}\")\n",
    "print(f\"Test samples: {len(test_IEMOCAP_dataset)}\")\n",
    "\n",
    "# Create IEMOCAP DataLoaders\n",
    "batch_size = 4\n",
    "train_IEMOCAP_dataloader = DataLoader(train_IEMOCAP_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_IEMOCAP_dataloader = DataLoader(val_IEMOCAP_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_IEMOCAP_dataloader = DataLoader(test_IEMOCAP_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "print(f\"////////////////////////////////////////////////////////////////////////////////////////////\")\n",
    "print(f\"Dataset RAVDESS\")\n",
    "print(f\"////////////////////////////////////////////////////////////////////////////////////////////\")\n",
    "\n",
    "# Usa il percorso trovato in precedenza, altrimenti fallback\n",
    "if ravdess_path and ravdess_path.exists():\n",
    "    dataset_RAVDESS_path = str(ravdess_path)\n",
    "    print(f\"âœ… Usando percorso trovato: {dataset_RAVDESS_path}\")\n",
    "else:\n",
    "    dataset_RAVDESS_path = '/kaggle/input/ravdess-emotional-speech-audio'\n",
    "    print(f\"âš ï¸  Percorso non trovato, usando fallback: {dataset_RAVDESS_path}\")\n",
    "\n",
    "# Create RAVDESS datasets\n",
    "train_RAVDESS_dataset = CustomRAVDESSDataset(dataset_root=dataset_RAVDESS_path, split='train')\n",
    "val_RAVDESS_dataset = CustomRAVDESSDataset(dataset_root=dataset_RAVDESS_path, split='validation')\n",
    "test_RAVDESS_dataset = CustomRAVDESSDataset(dataset_root=dataset_RAVDESS_path, split='test')\n",
    "\n",
    "print(f\"Train samples: {len(train_RAVDESS_dataset)}\")\n",
    "print(f\"Val samples: {len(val_RAVDESS_dataset)}\")\n",
    "print(f\"Test samples: {len(test_RAVDESS_dataset)}\")\n",
    "\n",
    "# Create RAVDESS DataLoaders\n",
    "batch_size = 4\n",
    "train_RAVDESS_dataloader = DataLoader(train_RAVDESS_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_RAVDESS_dataloader = DataLoader(val_RAVDESS_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_RAVDESS_dataloader = DataLoader(test_RAVDESS_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' import matplotlib.pyplot as plt\\nimport matplotlib.patches as mpatches\\n\\n# Colori per le emozioni\\nemotion_colors = {\\n    \\'neutral\\': \\'#4285F4\\',  # Blu\\n    \\'happy\\': \\'#34A853\\',    # Verde\\n    \\'sad\\': \\'#EA4335\\',      # Rosso\\n    \\'angry\\': \\'#FBBC04\\'     # Giallo/Arancione\\n}\\n\\nfig, axes = plt.subplots(3, 2, figsize=(16, 14))\\nfig.suptitle(\\'Log-Mel Spectrograms: IEMOCAP (Train vs Validation) vs RAVDESS (Train)\\', fontsize=16, fontweight=\\'bold\\')\\n\\n# ===== IEMOCAP - VALIDATION (ha le label) =====\\nprint(\"Loading IEMOCAP VALIDATION samples...\")\\niemocap_val_sample_1 = val_IEMOCAP_dataset[0]\\niemocap_val_sample_2 = val_IEMOCAP_dataset[1]\\n\\n# Plot IEMOCAP Validation Sample 1\\nim1 = axes[0, 0].imshow(iemocap_val_sample_1[\\'audio_features\\'].squeeze().numpy(), \\n                        aspect=\\'auto\\', origin=\\'lower\\', cmap=\\'viridis\\')\\naxes[0, 0].set_title(f\"IEMOCAP VALIDATION Sample 1\\nEmotion: {iemocap_val_sample_1[\\'emotion\\']} | Actor: {iemocap_val_sample_1[\\'actor_id\\']}\", \\n                     fontweight=\\'bold\\', color=\\'green\\')\\naxes[0, 0].set_ylabel(\\'Mel Frequency Bins\\')\\naxes[0, 0].set_xlabel(\\'Time Frames\\')\\nplt.colorbar(im1, ax=axes[0, 0], label=\\'dB\\')\\n\\n# Plot IEMOCAP Validation Sample 2\\nim2 = axes[0, 1].imshow(iemocap_val_sample_2[\\'audio_features\\'].squeeze().numpy(), \\n                        aspect=\\'auto\\', origin=\\'lower\\', cmap=\\'viridis\\')\\naxes[0, 1].set_title(f\"IEMOCAP VALIDATION Sample 2\\nEmotion: {iemocap_val_sample_2[\\'emotion\\']} | Actor: {iemocap_val_sample_2[\\'actor_id\\']}\", \\n                     fontweight=\\'bold\\', color=\\'green\\')\\naxes[0, 1].set_ylabel(\\'Mel Frequency Bins\\')\\naxes[0, 1].set_xlabel(\\'Time Frames\\')\\nplt.colorbar(im2, ax=axes[0, 1], label=\\'dB\\')\\n\\n# ===== IEMOCAP - TRAIN (NO label) =====\\nprint(\"Loading IEMOCAP TRAIN samples...\")\\niemocap_train_sample_1 = train_IEMOCAP_dataset[0]\\niemocap_train_sample_2 = train_IEMOCAP_dataset[1]\\n\\n# Plot IEMOCAP Train Sample 1\\nim3 = axes[1, 0].imshow(iemocap_train_sample_1[\\'audio_features\\'].squeeze().numpy(), \\n                        aspect=\\'auto\\', origin=\\'lower\\', cmap=\\'viridis\\')\\naxes[1, 0].set_title(f\"IEMOCAP TRAIN Sample 1\\nEmotion: {iemocap_train_sample_1[\\'emotion\\']} (NO LABEL) | Actor: {iemocap_train_sample_1[\\'actor_id\\']}\", \\n                     fontweight=\\'bold\\', color=\\'red\\')\\naxes[1, 0].set_ylabel(\\'Mel Frequency Bins\\')\\naxes[1, 0].set_xlabel(\\'Time Frames\\')\\nplt.colorbar(im3, ax=axes[1, 0], label=\\'dB\\')\\n\\n# Plot IEMOCAP Train Sample 2\\nim4 = axes[1, 1].imshow(iemocap_train_sample_2[\\'audio_features\\'].squeeze().numpy(), \\n                        aspect=\\'auto\\', origin=\\'lower\\', cmap=\\'viridis\\')\\naxes[1, 1].set_title(f\"IEMOCAP TRAIN Sample 2\\nEmotion: {iemocap_train_sample_2[\\'emotion\\']} (NO LABEL) | Actor: {iemocap_train_sample_2[\\'actor_id\\']}\", \\n                     fontweight=\\'bold\\', color=\\'red\\')\\naxes[1, 1].set_ylabel(\\'Mel Frequency Bins\\')\\naxes[1, 1].set_xlabel(\\'Time Frames\\')\\nplt.colorbar(im4, ax=axes[1, 1], label=\\'dB\\')\\n\\n# ===== RAVDESS - TRAIN =====\\nprint(\"Loading RAVDESS TRAIN samples...\")\\nravdess_sample_1 = train_RAVDESS_dataset[0]\\nravdess_sample_2 = train_RAVDESS_dataset[1]\\n\\n# Plot RAVDESS Sample 1\\nim5 = axes[2, 0].imshow(ravdess_sample_1[\\'audio_features\\'].squeeze().numpy(), \\n                        aspect=\\'auto\\', origin=\\'lower\\', cmap=\\'viridis\\')\\naxes[2, 0].set_title(f\"RAVDESS TRAIN Sample 1\\nEmotion: {ravdess_sample_1[\\'emotion\\']} | Actor: {ravdess_sample_1[\\'actor_id\\']}\", \\n                     fontweight=\\'bold\\')\\naxes[2, 0].set_ylabel(\\'Mel Frequency Bins\\')\\naxes[2, 0].set_xlabel(\\'Time Frames\\')\\nplt.colorbar(im5, ax=axes[2, 0], label=\\'dB\\')\\n\\n# Plot RAVDESS Sample 2\\nim6 = axes[2, 1].imshow(ravdess_sample_2[\\'audio_features\\'].squeeze().numpy(), \\n                        aspect=\\'auto\\', origin=\\'lower\\', cmap=\\'viridis\\')\\naxes[2, 1].set_title(f\"RAVDESS TRAIN Sample 2\\nEmotion: {ravdess_sample_2[\\'emotion\\']} | Actor: {ravdess_sample_2[\\'actor_id\\']}\", \\n                     fontweight=\\'bold\\')\\naxes[2, 1].set_ylabel(\\'Mel Frequency Bins\\')\\naxes[2, 1].set_xlabel(\\'Time Frames\\')\\nplt.colorbar(im6, ax=axes[2, 1], label=\\'dB\\')\\n\\nplt.tight_layout()\\nplt.show()\\n\\n# ===== Stampa delle statistiche =====\\nprint(\"\\n\" + \"=\"*80)\\nprint(\"SAMPLE DETAILS\")\\nprint(\"=\"*80)\\n\\nprint(\"\\nâœ… IEMOCAP VALIDATION (con label):\")\\nprint(\"\\nğŸ“Š Sample 1:\")\\nprint(f\"   Emotion: {iemocap_val_sample_1[\\'emotion\\']} (ID: {iemocap_val_sample_1[\\'emotion_id\\']})\")\\nprint(f\"   Actor: {iemocap_val_sample_1[\\'actor_id\\']}\")\\nprint(f\"   Spectrogram Shape: {iemocap_val_sample_1[\\'audio_features\\'].shape} (channels, mel_bins, time_frames)\")\\nprint(f\"   Min value: {iemocap_val_sample_1[\\'audio_features\\'].min().item():.2f} dB\")\\nprint(f\"   Max value: {iemocap_val_sample_1[\\'audio_features\\'].max().item():.2f} dB\")\\nprint(f\"   Mean value: {iemocap_val_sample_1[\\'audio_features\\'].mean().item():.2f} dB\")\\n\\nprint(\"\\nğŸ“Š Sample 2:\")\\nprint(f\"   Emotion: {iemocap_val_sample_2[\\'emotion\\']} (ID: {iemocap_val_sample_2[\\'emotion_id\\']})\")\\nprint(f\"   Actor: {iemocap_val_sample_2[\\'actor_id\\']}\")\\nprint(f\"   Spectrogram Shape: {iemocap_val_sample_2[\\'audio_features\\'].shape}\")\\nprint(f\"   Min value: {iemocap_val_sample_2[\\'audio_features\\'].min().item():.2f} dB\")\\nprint(f\"   Max value: {iemocap_val_sample_2[\\'audio_features\\'].max().item():.2f} dB\")\\nprint(f\"   Mean value: {iemocap_val_sample_2[\\'audio_features\\'].mean().item():.2f} dB\")\\n\\nprint(\"\\nâŒ IEMOCAP TRAIN (SENZA label - Unsupervised):\")\\nprint(\"\\nğŸ“Š Sample 1:\")\\nprint(f\"   Emotion: {iemocap_train_sample_1[\\'emotion\\']} | Audio ID: {iemocap_train_sample_1[\\'emotion_id\\']}\")\\nprint(f\"   Actor: {iemocap_train_sample_1[\\'actor_id\\']}\")\\nprint(f\"   Spectrogram Shape: {iemocap_train_sample_1[\\'audio_features\\'].shape}\")\\nprint(f\"   Min value: {iemocap_train_sample_1[\\'audio_features\\'].min().item():.2f} dB\")\\nprint(f\"   Max value: {iemocap_train_sample_1[\\'audio_features\\'].max().item():.2f} dB\")\\nprint(f\"   Mean value: {iemocap_train_sample_1[\\'audio_features\\'].mean().item():.2f} dB\")\\n\\nprint(\"\\nğŸ“Š Sample 2:\")\\nprint(f\"   Emotion: {iemocap_train_sample_2[\\'emotion\\']} | Audio ID: {iemocap_train_sample_2[\\'emotion_id\\']}\")\\nprint(f\"   Actor: {iemocap_train_sample_2[\\'actor_id\\']}\")\\nprint(f\"   Spectrogram Shape: {iemocap_train_sample_2[\\'audio_features\\'].shape}\")\\nprint(f\"   Min value: {iemocap_train_sample_2[\\'audio_features\\'].min().item():.2f} dB\")\\nprint(f\"   Max value: {iemocap_train_sample_2[\\'audio_features\\'].max().item():.2f} dB\")\\nprint(f\"   Mean value: {iemocap_train_sample_2[\\'audio_features\\'].mean().item():.2f} dB\")\\n\\nprint(\"\\nâœ… RAVDESS TRAIN (con label):\")\\nprint(\"\\nğŸ“Š Sample 1:\")\\nprint(f\"   Emotion: {ravdess_sample_1[\\'emotion\\']} (ID: {ravdess_sample_1[\\'emotion_id\\']})\")\\nprint(f\"   Actor: {ravdess_sample_1[\\'actor_id\\']}\")\\nprint(f\"   Spectrogram Shape: {ravdess_sample_1[\\'audio_features\\'].shape}\")\\nprint(f\"   Min value: {ravdess_sample_1[\\'audio_features\\'].min().item():.2f} dB\")\\nprint(f\"   Max value: {ravdess_sample_1[\\'audio_features\\'].max().item():.2f} dB\")\\nprint(f\"   Mean value: {ravdess_sample_1[\\'audio_features\\'].mean().item():.2f} dB\")\\n\\nprint(\"\\nğŸ“Š Sample 2:\")\\nprint(f\"   Emotion: {ravdess_sample_2[\\'emotion\\']} (ID: {ravdess_sample_2[\\'emotion_id\\']})\")\\nprint(f\"   Actor: {ravdess_sample_2[\\'actor_id\\']}\")\\nprint(f\"   Spectrogram Shape: {ravdess_sample_2[\\'audio_features\\'].shape}\")\\nprint(f\"   Min value: {ravdess_sample_2[\\'audio_features\\'].min().item():.2f} dB\")\\nprint(f\"   Max value: {ravdess_sample_2[\\'audio_features\\'].max().item():.2f} dB\")\\nprint(f\"   Mean value: {ravdess_sample_2[\\'audio_features\\'].mean().item():.2f} dB\")\\n\\nprint(\"\\n\" + \"=\"*80)\\nprint(\"âœ… Datasets are ready for training!\")\\nprint(\"=\"*80) '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizza alcuni spettri di log-mel e stampa le statistiche dei dataset\n",
    "\n",
    "\"\"\" import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Colori per le emozioni\n",
    "emotion_colors = {\n",
    "    'neutral': '#4285F4',  # Blu\n",
    "    'happy': '#34A853',    # Verde\n",
    "    'sad': '#EA4335',      # Rosso\n",
    "    'angry': '#FBBC04'     # Giallo/Arancione\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 14))\n",
    "fig.suptitle('Log-Mel Spectrograms: IEMOCAP (Train vs Validation) vs RAVDESS (Train)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# ===== IEMOCAP - VALIDATION (ha le label) =====\n",
    "print(\"Loading IEMOCAP VALIDATION samples...\")\n",
    "iemocap_val_sample_1 = val_IEMOCAP_dataset[0]\n",
    "iemocap_val_sample_2 = val_IEMOCAP_dataset[1]\n",
    "\n",
    "# Plot IEMOCAP Validation Sample 1\n",
    "im1 = axes[0, 0].imshow(iemocap_val_sample_1['audio_features'].squeeze().numpy(), \n",
    "                        aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[0, 0].set_title(f\"IEMOCAP VALIDATION Sample 1\\nEmotion: {iemocap_val_sample_1['emotion']} | Actor: {iemocap_val_sample_1['actor_id']}\", \n",
    "                     fontweight='bold', color='green')\n",
    "axes[0, 0].set_ylabel('Mel Frequency Bins')\n",
    "axes[0, 0].set_xlabel('Time Frames')\n",
    "plt.colorbar(im1, ax=axes[0, 0], label='dB')\n",
    "\n",
    "# Plot IEMOCAP Validation Sample 2\n",
    "im2 = axes[0, 1].imshow(iemocap_val_sample_2['audio_features'].squeeze().numpy(), \n",
    "                        aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[0, 1].set_title(f\"IEMOCAP VALIDATION Sample 2\\nEmotion: {iemocap_val_sample_2['emotion']} | Actor: {iemocap_val_sample_2['actor_id']}\", \n",
    "                     fontweight='bold', color='green')\n",
    "axes[0, 1].set_ylabel('Mel Frequency Bins')\n",
    "axes[0, 1].set_xlabel('Time Frames')\n",
    "plt.colorbar(im2, ax=axes[0, 1], label='dB')\n",
    "\n",
    "# ===== IEMOCAP - TRAIN (NO label) =====\n",
    "print(\"Loading IEMOCAP TRAIN samples...\")\n",
    "iemocap_train_sample_1 = train_IEMOCAP_dataset[0]\n",
    "iemocap_train_sample_2 = train_IEMOCAP_dataset[1]\n",
    "\n",
    "# Plot IEMOCAP Train Sample 1\n",
    "im3 = axes[1, 0].imshow(iemocap_train_sample_1['audio_features'].squeeze().numpy(), \n",
    "                        aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[1, 0].set_title(f\"IEMOCAP TRAIN Sample 1\\nEmotion: {iemocap_train_sample_1['emotion']} (NO LABEL) | Actor: {iemocap_train_sample_1['actor_id']}\", \n",
    "                     fontweight='bold', color='red')\n",
    "axes[1, 0].set_ylabel('Mel Frequency Bins')\n",
    "axes[1, 0].set_xlabel('Time Frames')\n",
    "plt.colorbar(im3, ax=axes[1, 0], label='dB')\n",
    "\n",
    "# Plot IEMOCAP Train Sample 2\n",
    "im4 = axes[1, 1].imshow(iemocap_train_sample_2['audio_features'].squeeze().numpy(), \n",
    "                        aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[1, 1].set_title(f\"IEMOCAP TRAIN Sample 2\\nEmotion: {iemocap_train_sample_2['emotion']} (NO LABEL) | Actor: {iemocap_train_sample_2['actor_id']}\", \n",
    "                     fontweight='bold', color='red')\n",
    "axes[1, 1].set_ylabel('Mel Frequency Bins')\n",
    "axes[1, 1].set_xlabel('Time Frames')\n",
    "plt.colorbar(im4, ax=axes[1, 1], label='dB')\n",
    "\n",
    "# ===== RAVDESS - TRAIN =====\n",
    "print(\"Loading RAVDESS TRAIN samples...\")\n",
    "ravdess_sample_1 = train_RAVDESS_dataset[0]\n",
    "ravdess_sample_2 = train_RAVDESS_dataset[1]\n",
    "\n",
    "# Plot RAVDESS Sample 1\n",
    "im5 = axes[2, 0].imshow(ravdess_sample_1['audio_features'].squeeze().numpy(), \n",
    "                        aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[2, 0].set_title(f\"RAVDESS TRAIN Sample 1\\nEmotion: {ravdess_sample_1['emotion']} | Actor: {ravdess_sample_1['actor_id']}\", \n",
    "                     fontweight='bold')\n",
    "axes[2, 0].set_ylabel('Mel Frequency Bins')\n",
    "axes[2, 0].set_xlabel('Time Frames')\n",
    "plt.colorbar(im5, ax=axes[2, 0], label='dB')\n",
    "\n",
    "# Plot RAVDESS Sample 2\n",
    "im6 = axes[2, 1].imshow(ravdess_sample_2['audio_features'].squeeze().numpy(), \n",
    "                        aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[2, 1].set_title(f\"RAVDESS TRAIN Sample 2\\nEmotion: {ravdess_sample_2['emotion']} | Actor: {ravdess_sample_2['actor_id']}\", \n",
    "                     fontweight='bold')\n",
    "axes[2, 1].set_ylabel('Mel Frequency Bins')\n",
    "axes[2, 1].set_xlabel('Time Frames')\n",
    "plt.colorbar(im6, ax=axes[2, 1], label='dB')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===== Stampa delle statistiche =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE DETAILS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nâœ… IEMOCAP VALIDATION (con label):\")\n",
    "print(\"\\nğŸ“Š Sample 1:\")\n",
    "print(f\"   Emotion: {iemocap_val_sample_1['emotion']} (ID: {iemocap_val_sample_1['emotion_id']})\")\n",
    "print(f\"   Actor: {iemocap_val_sample_1['actor_id']}\")\n",
    "print(f\"   Spectrogram Shape: {iemocap_val_sample_1['audio_features'].shape} (channels, mel_bins, time_frames)\")\n",
    "print(f\"   Min value: {iemocap_val_sample_1['audio_features'].min().item():.2f} dB\")\n",
    "print(f\"   Max value: {iemocap_val_sample_1['audio_features'].max().item():.2f} dB\")\n",
    "print(f\"   Mean value: {iemocap_val_sample_1['audio_features'].mean().item():.2f} dB\")\n",
    "\n",
    "print(\"\\nğŸ“Š Sample 2:\")\n",
    "print(f\"   Emotion: {iemocap_val_sample_2['emotion']} (ID: {iemocap_val_sample_2['emotion_id']})\")\n",
    "print(f\"   Actor: {iemocap_val_sample_2['actor_id']}\")\n",
    "print(f\"   Spectrogram Shape: {iemocap_val_sample_2['audio_features'].shape}\")\n",
    "print(f\"   Min value: {iemocap_val_sample_2['audio_features'].min().item():.2f} dB\")\n",
    "print(f\"   Max value: {iemocap_val_sample_2['audio_features'].max().item():.2f} dB\")\n",
    "print(f\"   Mean value: {iemocap_val_sample_2['audio_features'].mean().item():.2f} dB\")\n",
    "\n",
    "print(\"\\nâŒ IEMOCAP TRAIN (SENZA label - Unsupervised):\")\n",
    "print(\"\\nğŸ“Š Sample 1:\")\n",
    "print(f\"   Emotion: {iemocap_train_sample_1['emotion']} | Audio ID: {iemocap_train_sample_1['emotion_id']}\")\n",
    "print(f\"   Actor: {iemocap_train_sample_1['actor_id']}\")\n",
    "print(f\"   Spectrogram Shape: {iemocap_train_sample_1['audio_features'].shape}\")\n",
    "print(f\"   Min value: {iemocap_train_sample_1['audio_features'].min().item():.2f} dB\")\n",
    "print(f\"   Max value: {iemocap_train_sample_1['audio_features'].max().item():.2f} dB\")\n",
    "print(f\"   Mean value: {iemocap_train_sample_1['audio_features'].mean().item():.2f} dB\")\n",
    "\n",
    "print(\"\\nğŸ“Š Sample 2:\")\n",
    "print(f\"   Emotion: {iemocap_train_sample_2['emotion']} | Audio ID: {iemocap_train_sample_2['emotion_id']}\")\n",
    "print(f\"   Actor: {iemocap_train_sample_2['actor_id']}\")\n",
    "print(f\"   Spectrogram Shape: {iemocap_train_sample_2['audio_features'].shape}\")\n",
    "print(f\"   Min value: {iemocap_train_sample_2['audio_features'].min().item():.2f} dB\")\n",
    "print(f\"   Max value: {iemocap_train_sample_2['audio_features'].max().item():.2f} dB\")\n",
    "print(f\"   Mean value: {iemocap_train_sample_2['audio_features'].mean().item():.2f} dB\")\n",
    "\n",
    "print(\"\\nâœ… RAVDESS TRAIN (con label):\")\n",
    "print(\"\\nğŸ“Š Sample 1:\")\n",
    "print(f\"   Emotion: {ravdess_sample_1['emotion']} (ID: {ravdess_sample_1['emotion_id']})\")\n",
    "print(f\"   Actor: {ravdess_sample_1['actor_id']}\")\n",
    "print(f\"   Spectrogram Shape: {ravdess_sample_1['audio_features'].shape}\")\n",
    "print(f\"   Min value: {ravdess_sample_1['audio_features'].min().item():.2f} dB\")\n",
    "print(f\"   Max value: {ravdess_sample_1['audio_features'].max().item():.2f} dB\")\n",
    "print(f\"   Mean value: {ravdess_sample_1['audio_features'].mean().item():.2f} dB\")\n",
    "\n",
    "print(\"\\nğŸ“Š Sample 2:\")\n",
    "print(f\"   Emotion: {ravdess_sample_2['emotion']} (ID: {ravdess_sample_2['emotion_id']})\")\n",
    "print(f\"   Actor: {ravdess_sample_2['actor_id']}\")\n",
    "print(f\"   Spectrogram Shape: {ravdess_sample_2['audio_features'].shape}\")\n",
    "print(f\"   Min value: {ravdess_sample_2['audio_features'].min().item():.2f} dB\")\n",
    "print(f\"   Max value: {ravdess_sample_2['audio_features'].max().item():.2f} dB\")\n",
    "print(f\"   Mean value: {ravdess_sample_2['audio_features'].mean().item():.2f} dB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… Datasets are ready for training!\")\n",
    "print(\"=\"*80) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqo9Eh79yihI"
   },
   "source": [
    "# Step 4: Train your model and visualize training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Weights & Biases : Genera i grafici e compara gli esperimenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4-dxDQOFcdgX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpagliarellomatteo\u001b[0m (\u001b[33mpagliarellomatteo-politecnico-di-torino\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ['WANDB_API_KEY'] = '7ade30086de7899bed412e3eb5c2da065c146f90'\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "8q9OvEDHxmRv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Classe SimpleEarlyStopping pronta!\n",
      "Using device: cuda\n",
      "\n",
      "âœ… RAVDESS trovato: /kaggle/input/ravdess-emotional-speech-audio\n",
      "\n",
      "ğŸ“Š Statistiche del dataset RAVDESS:\n",
      "\n",
      "========================================\n",
      "ğŸ“Š ANALISI RAVDESS TRAINING SET\n",
      "========================================\n",
      "ğŸ”¹ Samples Totali: 1440\n",
      "ğŸ”¹ Attori (20): [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "   - Maschi:  10\n",
      "   - Femmine: 10\n",
      "\n",
      "ğŸ­ Distribuzione Emozioni:\n",
      "   - Angry     :  320 ( 22.2%) â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   - Happy     :  320 ( 22.2%) â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   - Neutral   :  480 ( 33.3%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   - Sad       :  320 ( 22.2%) â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "----------------------------------------\n",
      "ğŸ“Š Statistiche del dataset RAVDESS:\n",
      "\n",
      "========================================\n",
      "ğŸ“Š ANALISI RAVDESS VALIDATION SET\n",
      "========================================\n",
      "ğŸ”¹ Samples Totali: 144\n",
      "ğŸ”¹ Attori (2): [21, 22]\n",
      "   - Maschi:  1\n",
      "   - Femmine: 1\n",
      "\n",
      "ğŸ­ Distribuzione Emozioni:\n",
      "   - Angry     :   32 ( 22.2%) â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   - Happy     :   32 ( 22.2%) â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   - Neutral   :   48 ( 33.3%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   - Sad       :   32 ( 22.2%) â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "----------------------------------------\n",
      "Train samples: 1440\n",
      "Val samples: 144\n",
      "\n",
      "================================================================================\n",
      "ğŸ—ï¸ ARCHITETTURA DEL MODELLO\n",
      "================================================================================\n",
      "CRNN_BiLSTM(\n",
      "  (block1): Sequential(\n",
      "    (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (block2): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (block3): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (block4): Sequential(\n",
      "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (projection): Linear(in_features=8192, out_features=128, bias=True)\n",
      "  (lstm): LSTM(128, 128, batch_first=True, bidirectional=True)\n",
      "  (attention_linear): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (classifier): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n",
      "================================================================================\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpagliarellomatteo\u001b[0m (\u001b[33mpagliarellomatteo-politecnico-di-torino\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/speech-emotion-recognition-25/wandb/run-20260118_181449-2o57sidx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrain_20260118_191449\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/pagliarellomatteo-politecnico-di-torino/speech-emotion-recognition\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pagliarellomatteo-politecnico-di-torino/speech-emotion-recognition/runs/2o57sidx\u001b[0m\n",
      "\n",
      "================================================================================\n",
      "ğŸ”§ IPERPARAMETRI DI TRAINING\n",
      "================================================================================\n",
      "Device:                cuda\n",
      "Batch Size:            32\n",
      "Learning Rate:         0.0005\n",
      "Weight Decay (L2):     0.0001\n",
      "Number of Epochs:      100\n",
      "Early Stopping Patience: 10\n",
      "\n",
      "Modello:\n",
      "  - Num Classes:       4\n",
      "  - Time Steps:        200\n",
      "  - Mel Bands:         128\n",
      "\n",
      "Optimizer:             Adam\n",
      "Loss Function:         CrossEntropyLoss\n",
      "Train Samples:         1440\n",
      "Val Samples:           144\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "Training:   0% 0/45 [00:00<?, ?it/s]\n",
      "ğŸ“Š Calcolando statistiche durata POST-TRIMMING per split 'train'...\n",
      "   288/1440 file processati...\n",
      "   576/1440 file processati...\n",
      "   864/1440 file processati...\n",
      "   1152/1440 file processati...\n",
      "   1440/1440 file processati...\n",
      "âœ… Media: 2.19s (35043 campioni)\n",
      "âœ… Massimo: 4.40s (70471 campioni)\n",
      "\n",
      "                                                        \n",
      "ğŸ“Š Calcolando statistiche durata POST-TRIMMING per split 'validation'...\n",
      "   28/144 file processati...\n",
      "   56/144 file processati...\n",
      "   84/144 file processati...\n",
      "   112/144 file processati...\n",
      "   140/144 file processati...\n",
      "âœ… Media: 2.22s (35541 campioni)\n",
      "âœ… Massimo: 3.07s (49152 campioni)\n",
      "\n",
      "Train Loss: 1.2365 | Train Acc: 41.74%\n",
      "Val Loss:   1.2970 | Val Acc:   44.44%\n",
      ">>> Model Saved!\n",
      "ğŸ“Š Best val loss: 1.2970\n",
      "\n",
      "Epoch 2/100\n",
      "Train Loss: 1.0679 | Train Acc: 50.42%                   \n",
      "Val Loss:   1.0157 | Val Acc:   54.17%\n",
      ">>> Model Saved!\n",
      "âœ“ Migliorato! New best: 1.0157\n",
      "\n",
      "Epoch 3/100\n",
      "Train Loss: 1.0262 | Train Acc: 53.19%                   \n",
      "Val Loss:   0.9541 | Val Acc:   62.50%\n",
      ">>> Model Saved!\n",
      "âœ“ Migliorato! New best: 0.9541\n",
      "\n",
      "Epoch 4/100\n",
      "Train Loss: 1.0139 | Train Acc: 53.61%                   \n",
      "Val Loss:   0.9657 | Val Acc:   48.61%\n",
      "âš ï¸ Nessun miglioramento (1/10)\n",
      "\n",
      "Epoch 5/100\n",
      "Train Loss: 0.9294 | Train Acc: 59.79%                   \n",
      "Val Loss:   0.8293 | Val Acc:   70.83%\n",
      ">>> Model Saved!\n",
      "âœ“ Migliorato! New best: 0.8293\n",
      "\n",
      "Epoch 6/100\n",
      "Train Loss: 0.8889 | Train Acc: 61.60%                   \n",
      "Val Loss:   0.7549 | Val Acc:   68.06%\n",
      "âœ“ Migliorato! New best: 0.7549\n",
      "\n",
      "Epoch 7/100\n",
      "Train Loss: 0.8599 | Train Acc: 63.96%                   \n",
      "Val Loss:   1.4718 | Val Acc:   37.50%\n",
      "âš ï¸ Nessun miglioramento (1/10)\n",
      "\n",
      "Epoch 8/100\n",
      "Train Loss: 0.8530 | Train Acc: 62.29%                   \n",
      "Val Loss:   0.7608 | Val Acc:   62.50%\n",
      "âš ï¸ Nessun miglioramento (2/10)\n",
      "\n",
      "Epoch 9/100\n",
      "Train Loss: 0.7734 | Train Acc: 69.10%                   \n",
      "Val Loss:   0.7733 | Val Acc:   68.06%\n",
      "âš ï¸ Nessun miglioramento (3/10)\n",
      "\n",
      "Epoch 10/100\n",
      "Train Loss: 0.7478 | Train Acc: 69.72%                   \n",
      "Val Loss:   1.0384 | Val Acc:   63.89%\n",
      "âš ï¸ Nessun miglioramento (4/10)\n",
      "\n",
      "Epoch 11/100\n",
      "Train Loss: 0.7040 | Train Acc: 70.97%                   \n",
      "Val Loss:   0.8686 | Val Acc:   68.06%\n",
      "âš ï¸ Nessun miglioramento (5/10)\n",
      "\n",
      "Epoch 12/100\n",
      "Train Loss: 0.6486 | Train Acc: 73.61%                   \n",
      "Val Loss:   0.7258 | Val Acc:   54.17%\n",
      "âœ“ Migliorato! New best: 0.7258\n",
      "\n",
      "Epoch 13/100\n",
      "Train Loss: 0.6284 | Train Acc: 75.14%                   \n",
      "Val Loss:   1.0177 | Val Acc:   55.56%\n",
      "âš ï¸ Nessun miglioramento (1/10)\n",
      "\n",
      "Epoch 14/100\n",
      "Train Loss: 0.5714 | Train Acc: 78.96%                   \n",
      "Val Loss:   0.7221 | Val Acc:   61.11%\n",
      "âœ“ Migliorato! New best: 0.7221\n",
      "\n",
      "Epoch 15/100\n",
      "Train Loss: 0.5727 | Train Acc: 76.53%                   \n",
      "Val Loss:   0.8590 | Val Acc:   68.06%\n",
      "âš ï¸ Nessun miglioramento (1/10)\n",
      "\n",
      "Epoch 16/100\n",
      "Train Loss: 0.5137 | Train Acc: 78.96%                   \n",
      "Val Loss:   1.1982 | Val Acc:   59.72%\n",
      "\n",
      "ğŸ”„ Attivazione SWA da epoch 16\n",
      "âš ï¸ Nessun miglioramento (2/10)\n",
      "\n",
      "Epoch 17/100\n",
      "Train Loss: 0.4707 | Train Acc: 81.67%                   \n",
      "Val Loss:   0.7743 | Val Acc:   59.72%\n",
      "âš ï¸ Nessun miglioramento (3/10)\n",
      "\n",
      "Epoch 18/100\n",
      "Train Loss: 0.4411 | Train Acc: 83.47%                   \n",
      "Val Loss:   0.6570 | Val Acc:   75.00%\n",
      ">>> Model Saved!\n",
      "âœ“ Migliorato! New best: 0.6570\n",
      "\n",
      "Epoch 19/100\n",
      "Train Loss: 0.4390 | Train Acc: 83.06%                   \n",
      "Val Loss:   0.5825 | Val Acc:   79.17%\n",
      ">>> Model Saved!\n",
      "âœ“ Migliorato! New best: 0.5825\n",
      "\n",
      "Epoch 20/100\n",
      "Train Loss: 0.3500 | Train Acc: 87.29%                   \n",
      "Val Loss:   0.7815 | Val Acc:   70.83%\n",
      "\n",
      "ğŸ“Š Valutazione SWA model...\n",
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "SWA Val Loss: 0.5438 | SWA Val Acc: 81.94%\n",
      ">>> SWA Model Saved!\n",
      "âš ï¸ Nessun miglioramento (1/10)\n",
      "\n",
      "Epoch 21/100\n",
      "Train Loss: 0.3227 | Train Acc: 87.92%                   \n",
      "Val Loss:   0.8468 | Val Acc:   72.22%\n",
      "âš ï¸ Nessun miglioramento (2/10)\n",
      "\n",
      "Epoch 22/100\n",
      "Train Loss: 0.3193 | Train Acc: 88.68%                   \n",
      "Val Loss:   0.6527 | Val Acc:   79.17%\n",
      "âš ï¸ Nessun miglioramento (3/10)\n",
      "\n",
      "Epoch 23/100\n",
      "Train Loss: 0.2508 | Train Acc: 90.56%                   \n",
      "Val Loss:   0.6827 | Val Acc:   77.78%\n",
      "âš ï¸ Nessun miglioramento (4/10)\n",
      "\n",
      "Epoch 24/100\n",
      "Train Loss: 0.2537 | Train Acc: 91.25%                    \n",
      "Val Loss:   0.7355 | Val Acc:   75.00%\n",
      "âš ï¸ Nessun miglioramento (5/10)\n",
      "\n",
      "Epoch 25/100\n",
      "Train Loss: 0.2419 | Train Acc: 92.64%                    \n",
      "Val Loss:   0.6566 | Val Acc:   77.78%\n",
      "\n",
      "ğŸ“Š Valutazione SWA model...\n",
      "SWA Val Loss: 0.5403 | SWA Val Acc: 83.33%\n",
      ">>> SWA Model Saved!\n",
      "âš ï¸ Nessun miglioramento (6/10)\n",
      "\n",
      "Epoch 26/100\n",
      "Train Loss: 0.1826 | Train Acc: 93.33%                    \n",
      "Val Loss:   1.0588 | Val Acc:   70.83%\n",
      "âš ï¸ Nessun miglioramento (7/10)\n",
      "\n",
      "Epoch 27/100\n",
      "Train Loss: 0.2001 | Train Acc: 93.75%                    \n",
      "Val Loss:   0.6518 | Val Acc:   73.61%\n",
      "âš ï¸ Nessun miglioramento (8/10)\n",
      "\n",
      "Epoch 28/100\n",
      "Train Loss: 0.2111 | Train Acc: 92.22%                    \n",
      "Val Loss:   0.6811 | Val Acc:   76.39%\n",
      "âš ï¸ Nessun miglioramento (9/10)\n",
      "\n",
      "Epoch 29/100\n",
      "Train Loss: 0.1924 | Train Acc: 93.33%                    \n",
      "Val Loss:   0.7968 | Val Acc:   75.00%\n",
      "âš ï¸ Nessun miglioramento (10/10)\n",
      "ğŸ›‘ STOP! Nessun miglioramento per 10 epoche\n",
      "\n",
      "â¹ï¸ Early stopping attivato dopo 29 epoche\n",
      "\n",
      "ğŸ”„ Valutazione finale SWA model...\n",
      "Final SWA Val Loss: 0.5666 | Final SWA Val Acc: 81.94%\n",
      "\n",
      "================================================================================\n",
      "âœ… Training Complete!\n",
      "Best Validation Accuracy (Regular): 79.17%\n",
      "Best Validation Accuracy (SWA):     83.33%\n",
      "\n",
      "ğŸ’¡ SWA Improvement: +4.17%\n",
      "\n",
      "ğŸ“¦ Checkpoints salvati in ./checkpoints/:\n",
      "   â€¢ best_model.pth     â†’ Modello standard\n",
      "   â€¢ best_swa_model.pth â†’ Modello SWA (raccomandato)\n",
      "================================================================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            epoch â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: swa_val_accuracy â–â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     swa_val_loss â–ˆâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_accuracy â–â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss â–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_accuracy â–‚â–„â–…â–ƒâ–‡â–†â–â–…â–†â–…â–†â–„â–„â–…â–†â–…â–…â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val_loss â–‡â–„â–„â–„â–ƒâ–‚â–ˆâ–‚â–ƒâ–…â–ƒâ–‚â–„â–‚â–ƒâ–†â–ƒâ–‚â–â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–…â–‚â–‚â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            epoch 29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: swa_val_accuracy 83.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     swa_val_loss 0.54025\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_accuracy 93.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss 0.1924\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_accuracy 75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val_loss 0.79678\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run \u001b[33mtrain_20260118_191449\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pagliarellomatteo-politecnico-di-torino/speech-emotion-recognition/runs/2o57sidx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pagliarellomatteo-politecnico-di-torino/speech-emotion-recognition\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20260118_181449-2o57sidx/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python train.py --model CRNN_BiLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DyQo3klIymlz"
   },
   "source": [
    "# Step 5: Evaluate your model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9goKvp4jxk4j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Timestamp valutazione: 20260118_193445\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpagliarellomatteo\u001b[0m (\u001b[33mpagliarellomatteo-politecnico-di-torino\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/speech-emotion-recognition-25/wandb/run-20260118_183445-j3zakv64\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meval_20260118_193445\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/pagliarellomatteo-politecnico-di-torino/speech-emotion-recognition\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pagliarellomatteo-politecnico-di-torino/speech-emotion-recognition/runs/j3zakv64\u001b[0m\n",
      "âœ… RAVDESS trovato: /kaggle/input/ravdess-emotional-speech-audio\n",
      "\n",
      "Loading RAVDESS test set...\n",
      "ğŸ“Š Statistiche del dataset RAVDESS:\n",
      "\n",
      "========================================\n",
      "ğŸ“Š ANALISI RAVDESS TEST SET\n",
      "========================================\n",
      "ğŸ”¹ Samples Totali: 144\n",
      "ğŸ”¹ Attori (2): [23, 24]\n",
      "   - Maschi:  1\n",
      "   - Femmine: 1\n",
      "\n",
      "ğŸ­ Distribuzione Emozioni:\n",
      "   - Angry     :   32 ( 22.2%) â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   - Happy     :   32 ( 22.2%) â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   - Neutral   :   48 ( 33.3%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   - Sad       :   32 ( 22.2%) â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "----------------------------------------\n",
      "âœ… Test samples: 144\n",
      "\n",
      "Loading model...\n",
      "âœ… Modello caricato da checkpoints/best_model.pth\n",
      "\n",
      "================================================================================\n",
      "TESTING IN CORSO...\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Calcolando statistiche durata POST-TRIMMING per split 'test'...\n",
      "   28/144 file processati...\n",
      "   56/144 file processati...\n",
      "   84/144 file processati...\n",
      "   112/144 file processati...\n",
      "   140/144 file processati...\n",
      "âœ… Media: 2.04s (32583 campioni)\n",
      "âœ… Massimo: 3.65s (58368 campioni)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š METRICHE DI VALUTAZIONE - FASE 1 (RAVDESS BASELINE)\n",
      "================================================================================\n",
      "\n",
      "ğŸ¯ METRICHE PRINCIPALI:\n",
      "   âœ… Accuracy:           52.78%\n",
      "   ğŸ“ˆ Macro-Avg F1:       0.5428\n",
      "   ğŸ“Š Weighted-Avg F1:    0.5103\n",
      "\n",
      "ğŸ“‹ CLASS DISTRIBUTION (Test Set):\n",
      "   neutral   :  48 samples ( 33.3%)\n",
      "   happy     :  32 samples ( 22.2%)\n",
      "   sad       :  32 samples ( 22.2%)\n",
      "   angry     :  32 samples ( 22.2%)\n",
      "\n",
      "ğŸ­ DETAILED CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.50      0.17      0.25        48\n",
      "       happy       0.77      0.62      0.69        32\n",
      "         sad       0.30      0.62      0.41        32\n",
      "       angry       0.78      0.88      0.82        32\n",
      "\n",
      "    accuracy                           0.53       144\n",
      "   macro avg       0.59      0.57      0.54       144\n",
      "weighted avg       0.58      0.53      0.51       144\n",
      "\n",
      "Figure(1000x800)\n",
      "\n",
      "================================================================================\n",
      "âœ… Evaluation Complete!\n",
      "   Final Accuracy: 52.78%\n",
      "   Macro-Avg F1:   0.5428\n",
      "   Weighted-Avg F1: 0.5103\n",
      "   Risultati loggati su W&B\n",
      "================================================================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading artifact run-j3zakv64-classification_report (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading artifact run-j3zakv64-classification_report (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading artifact run-j3zakv64-classification_report (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading artifact run-j3zakv64-classification_report (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading artifact run-j3zakv64-classification_report (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading artifact run-j3zakv64-classification_report (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading artifact run-j3zakv64-classification_report (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading artifact run-j3zakv64-classification_report (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading artifact run-j3zakv64-classification_report (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading artifact run-j3zakv64-classification_report (1.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading artifact run-j3zakv64-classification_report (1.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading artifact run-j3zakv64-classification_report (1.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading artifact run-j3zakv64-classification_report (1.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading artifact run-j3zakv64-classification_report (1.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    test_accuracy â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    test_macro_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_weighted_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    test_accuracy 0.52778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    test_macro_f1 0.54284\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_weighted_f1 0.5103\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run \u001b[33meval_20260118_193445\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pagliarellomatteo-politecnico-di-torino/speech-emotion-recognition/runs/j3zakv64\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pagliarellomatteo-politecnico-di-torino/speech-emotion-recognition\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20260118_183445-j3zakv64/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python eval.py --model CRNN_BiLSTM --checkpoint checkpoints/best_model.pth"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
